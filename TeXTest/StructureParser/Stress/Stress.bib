@Preamble{
	"\hyphenation{}"
}

@BOOK{Clote:CMB2000,
  AUTHOR = {Peter Clote and R. Backofen},
  TITLE = {Computational Molecular Biology: An Introduction},
  YEAR = {2000},
  MONTH = {Aug},
  PUBLISHER = {John Wiley & Sons, Ltd},
  ISBN = {0-471-87251-2},
  ISBN = {0-471-87252-0},
  DOCUMENTURL = {http://www.cs.bc.edu/~clote/computationalBiologyTableOfContents.html},
  BOSTONCOLLEGECOMPUTERSCIENCE = {yes}
}

@BOOK{Clote:BFMC2000,
  AUTHOR = {Peter Clote and E. Kranakis},
  TITLE = {Boolean Functions and Models of Computation},
  YEAR = {2000},
  MONTH = {Sep},
  PUBLISHER = {Springer-Verlag},
  BOSTONCOLLEGECOMPUTERSCIENCE = {yes}
}

@BOOK{Clote:CSL2000,
  AUTHOR = {Peter Clote},
  TITLE = {Computer Science Logic},
  YEAR = {2000},
  MONTH = {Aug},
  PUBLISHER = {Springer-Verlag},
  BOSTONCOLLEGECOMPUTERSCIENCE = {yes}
}

@BOOK{Clote:TR2000,
  AUTHOR = {R. Fraissé},
  TITLE = {Theory of Relations},
  YEAR = {1986},
  PUBLISHER = {North Holland},
  NOTE = {Research monograph translation by Peter Clote: French to English},
  BOSTONCOLLEGECOMPUTERSCIENCE = {yes}
}

@BOOK{Clote:APTCC1993,
  AUTHOR = {Peter Clote and J. Krajicek},
  TITLE = {Arithmetic, Proof Theory and Computational Complexity},
  YEAR = {1993},
  PUBLISHER = {Oxford University Press},
  BOSTONCOLLEGECOMPUTERSCIENCE = {yes}
}

@BOOK{Clote:FMII1995,
  AUTHOR = {Peter Clote and J. Remmel},
  TITLE = {Feasible Mathematics II},
  YEAR = {1995},
  PUBLISHER = {Birkhäuser Inc},
  BOSTONCOLLEGECOMPUTERSCIENCE = {yes}
}

@ARTICLE{Clote:RMTAGT1998,
  AUTHOR = {Peter Clote and J. Hirst},
  TITLE = {Reverse mathematics of some topics from algorithmic graph theory},
  JOURNAL = {Fundamenta Mathematica},
  VOLUME = {157},
  NUMBER = {1},
  PAGES = {1-13},
  YEAR = {1998},
  BOSTONCOLLEGECOMPUTERSCIENCE = {yes}
}

@ARTICLE{Clote:NDSRM1997,
  AUTHOR = {Peter Clote},
  TITLE = {Nondeterministic stack register machines},
  JOURNAL = {Theoretical Computer Science},
  VOLUME = {178},
  PAGES = {37-76},
  YEAR = {1997},
  MONTH = {Jun},
  PS = {http://www.cs.bc.edu/~clote/pub/stackRegister.ps},
  BOSTONCOLLEGECOMPUTERSCIENCE = {yes}
}

@ARTICLE{Clote:NMC1996,
  AUTHOR = {Peter Clote},
  TITLE = {Note on monotonic complexity of 2-REF},
  JOURNAL = {Information Processing Letters},
  VOLUME = {57},
  PAGES = {117-123},
  YEAR = {1996},
  PS = {http://www.cs.bc.edu/~clote/pub/monotonic.ps},
  BOSTONCOLLEGECOMPUTERSCIENCE = {yes}
}

@ARTICLE{Clote:CPCTL1995,
  AUTHOR = {Peter Clote and S. Buss},
  TITLE = {Cutting planes, connectivity and threshold logic},
  JOURNAL = {Archive for Mathematical Logic},
  VOLUME = {35},
  PAGES = {33-62},
  YEAR = {1995},
  BOSTONCOLLEGECOMPUTERSCIENCE = {yes}
}

@ARTICLE{Clote:CPFP1995,
  AUTHOR = {Peter Clote},
  TITLE = {Cutting plane and Frege proofs},
  JOURNAL = {Information and Computation},
  VOLUME = {121},
  NUMBER = {1},
  PAGES = {103-122},
  YEAR = {1995},
  BOSTONCOLLEGECOMPUTERSCIENCE = {yes}
}

@ARTICLE{Clote:ACSAC1992,
  AUTHOR = {Peter Clote},
  TITLE = {ALOGTIME and a conjecture of S.A. Cook},
  JOURNAL = {Annals of Mathematics and Artificial Intelligence},
  VOLUME = {6},
  PAGES = {57-106},
  YEAR = {1992},
  BOSTONCOLLEGECOMPUTERSCIENCE = {yes}
}

@ARTICLE{Clote:BANCALN1992,
  AUTHOR = {Peter Clote and G. Takeuti},
  TITLE = {Bounded arithmetic for NC, ALOGTIME, L and NL},
  JOURNAL = {Annals of Pure and Applied Logic},
  VOLUME = {56},
  PAGES = {73-117},
  YEAR = {1992},
  BOSTONCOLLEGECOMPUTERSCIENCE = {yes}
}

@ARTICLE{Clote:TSHBPP1992,
  AUTHOR = {Peter Clote},
  TITLE = {A time-space hierarchy between P and PSPACE},
  JOURNAL = {Mathematical Systems Theory},
  VOLUME = {25},
  PAGES = {77-92},
  YEAR = {1992},
  BOSTONCOLLEGECOMPUTERSCIENCE = {yes}
}

@ARTICLE{Clote:SFCSA1991,
  AUTHOR = {Peter Clote and P. Hájek and J. Paris},
  TITLE = {On some formalized consistency statements in arithmetic},
  JOURNAL = {Archive for Mathematical Logic },
  VOLUME = {30},
  NUMBER = {4},
  PAGES = {201-221},
  YEAR = {1991},
  BOSTONCOLLEGECOMPUTERSCIENCE = {yes}
}

@ARTICLE{Clote:BFIGPC1991,
  AUTHOR = {Peter Clote and E. Kranakis},
  TITLE = {Boolean functions, invariance groups, and parallel complexity},
  JOURNAL = {SIAM J. Comput},
  VOLUME = {20},
  NUMBER = {3},
  PAGES = {553-590},
  YEAR = {1991},
  BOSTONCOLLEGECOMPUTERSCIENCE = {yes}
}

@ARTICLE{Clote:MSLO1989,
  AUTHOR = {Peter Clote},
  TITLE = {The metamathematics of scattered linear orderings},
  JOURNAL = {Archive for Mathematical Logic},
  VOLUME = {29},
  PAGES = {9-20},
  YEAR = {1989},
  BOSTONCOLLEGECOMPUTERSCIENCE = {yes}
}

@ARTICLE{Clote:NMST1986,
  AUTHOR = {Peter Clote},
  TITLE = {A note on the MacDowell-Specker Theorem},
  JOURNAL = {Fundamenta Mathematicae},
  VOLUME = {127},
  PAGES = {163-170},
  YEAR = {1986},
  BOSTONCOLLEGECOMPUTERSCIENCE = {yes}
}

@ARTICLE{Clote:MPC1986,
  AUTHOR = {Peter Clote and D. Cenzer and R. Smith and R. Soare and S. Wainer},
  TITLE = {Members of PI01 classes},
  JOURNAL = {Annals of Pure and Applied Logic},
  VOLUME = {31},
  PAGES = {145-161},
  YEAR = {1986},
  BOSTONCOLLEGECOMPUTERSCIENCE = {yes}
}

@InProceedings{wisniewski:in-place,
  author = {Leonard F. Wisniewski},
  title = {Structured Permuting in Place on Parallel Disk Systems},
  booktitle = {Proceedings of the Fourth Workshop on Input/Output in Parallel
  and Distributed Systems},
  year = {1996},
  month = {May},
  pages = {128--139},
  publisher = {ACM Press},
  address = {Philadelphia},
  keywords = {parallel I/O, parallel I/O algorithm, permutation, out-of-core,
  pario-bib},
  abstract = {The ability to perform permutations of large data sets in place
  reduces the amount of necessary available disk storage. The simplest way to
  perform a permutation often is to read the records of a data set from a
  source portion of data storage, permute them in memory, and write them to a
  separate target portion of the same size. It can be quite expensive, however,
  to provide disk storage that is twice the size of very large data sets.
  Permuting in place reduces the expense by using only a small amount of extra
  disk storage beyond the size of the data set. \par
  \newcommand{\ceil}[1]{\lceil #1\rceil} \newcommand{\rank}[1]{\mathop{\rm
  rank}\nolimits #1} This paper features in-place algorithms for commonly used
  structured permutations. We have developed an asymptotically optimal
  algorithm for performing BMMC (bit-matrix-multiply/complement) permutations
  in place that requires at most $\frac{2N}{BD}\left(
  2\ceil{\frac{\rank{\gamma}}{\lg (M/B)}} + \frac{7}{2}\right)$ parallel disk
  accesses, as long as $M \geq 2BD$, where $N$ is the number of records in the
  data set, $M$ is the number of records that can fit in memory, $D$ is the
  number of disks, $B$ is the number of records in a block, and $\gamma$ is the
  lower left $\lg (N/B) \times \lg B$ submatrix of the characteristic matrix
  for the permutation. This algorithm uses $N+M$ records of disk storage and
  requires only a constant factor more parallel disk accesses and insignificant
  additional computation than a previously published asymptotically optimal
  algorithm that uses $2N$ records of disk storage. \par We also give
  algorithms to perform mesh and torus permutations on a $d$-dimensional mesh.
  The in-place algorithm for mesh permutations requires at most $3\ceil{N/BD}$
  parallel I/Os and the in-place algorithm for torus permutations uses at most
  $4dN/BD$ parallel I/Os. The algorithms for mesh and torus permutations
  require no extra disk space as long as the memory size $M$ is at least $3BD$.
  The torus algorithm improves upon the previous best algorithm in terms of
  both time and space.}
}

@COMMENT{{This file has been generated by bib2bib 1.66}}

@MASTERSTHESIS{hahsler:Hahsler1997,
  AUTHOR = {Michael Hahsler},
  TITLE = {Software {P}atterns: Pinw\"ande},
  SCHOOL = {Wirtschaftsuniversit\"at Wien},
  ADDRESS = {Augasse 2--6, A 1090 Wien, Österreich},
  YEAR = 1997,
  TYPE = {Diplomarbeit},
  NUMBER = {},
  MONTH = NOV,
  NOTE = {},
  HTML = {http://wwwai.wu-wien.ac.at/~hahsler/research/diplomarbeit/dipl/dipl.html},
  PDF = {http://wwwai.wu-wien.ac.at/~hahsler/research/diplomarbeit/dipl/pinwand_patterns.pdf},
  ABSTRACT = {Diese Arbeit beschäftigt sich mit dem Pattern-Ansatz für die
    Architektur von Software. Nach einer kurzen Darstellung des Ansatzes werden
    das Pinwand-Pattern und seine Varianten beschrieben. Pinwände werden
    verwendet, um Informationen zu sammeln und Interessierten zur Verfügung zu
    stellen. Sie finden unter anderem in den folgenden Bereichen Anwendung: 
  Groupware-Anwendungen, Conferencing Systeme, Diskussionsforen und Virtuelle
  Bibliotheken.  }
}

@PHDTHESIS{hahsler:Hahsler2001,
  AUTHOR = {Michael Hahsler},
  TITLE = {Analyse Patterns im Softwareentwicklungsprozeß 
    mit Beispielen für Informationsmanagement und deren Anwendungen für 
    die Virtuellen Universit\"at der Wirtschaftsuniversit\"at Wien},
  SCHOOL = {Wirtschaftsuniversit\"at Wien},
  ADDRESS = {Augasse 2-6, A 1090 Wien, Österreich},
  YEAR = 2001,
  TYPE = {Dissertation},
  NUMBER = {},
  MONTH = JAN,
  NOTE = {},
  URL = { http://epub.wu-wien.ac.at/dyn/openURL?id=oai:epub.wu-wien.ac.at:epub-wu-01_60},
  HTML = {http://wwwai.wu-wien.ac.at/~hahsler/research/diss/diss/diss.html},
  PDF = {http://wwwai.wu-wien.ac.at/~hahsler/research/diss/diss.pdf},
  PS = {http://wwwai.wu-wien.ac.at/~hahsler/research/diss/diss.ps},
  ABSTRACT = {Diese Arbeit beschäftigt sich mit Analyse Patterns,
  der  Anwendung von Patterns in der Analysephase der Softwareentwicklung.
  In der Designphase werden Patterns seit einigen Jahren
  eingesetzt, um Expertenwissen und Wiederverwendbarkeit
  in den Designprozeß einfließen zu lassen.
  Es existiert bereits eine Fülle an solchen Design Patterns.
  Die Analysephase ist ein neuer Anwendungsbereich für Patterns, der
  bisher in der Literatur noch nicht ausreichend behandelt wurde.
  In dieser Arbeit wird die Anwendung des Pattern-Ansatzes in der 
  Analysephase aufgearbeitet und konkretisiert. Analyse Patterns unterstützen
  den gesamten Softwareentwicklungsprozeß und helfen
  bekannte Probleme während der Analysephase zu lösen. Dadurch können 
  Zeit und Kosten bei der Entwicklung neuer Softwaresysteme eingespart werden.
  Diese Eigenschaften von Analyse Patterns werden anhand
  konkreter Beispiele in einer Case Study nachgewiesen. Diese Case Study 
  beschreibt den Einsatz von in dieser Arbeit entwickelten Analyse Pattern
  für Informationsmanagement anhand des Projekts Virtuelle Universität 
  der Wirtschaftsuniversität Wien, in dem ein Internet-Informationsbroker 
  zur Unterstützung von Lehre und Forschung realisiert wird.
  Die Erfahrungen aus diesem Projekt werden untersucht, und
  die Auswirkungen der Analyse Patterns auf Wiederverwendung bei der 
  Softwareentwicklung und auf die Akzeptanz des resultierenden 
  Systems werden präsentiert.}
}

@TECHREPORT{hahsler:GeyerSchulz2001d,
  AUTHOR = {Andreas Geyer-Schulz and Michael Hahsler},
  TITLE = {Software Engineering with Analysis Patterns},
  YEAR = 2001,
  MONTH = NOV,
  ADDRESS = {Augasse 2--6, 1090 Wien, Austria},
  INSTITUTION = {Working Papers on Information Processing and Information Management, Institut f\"ur Informationsverarbeitung und -wirtschaft, Wirschaftsuniversit\"at Wien},
  TYPE = {Working Paper},
  NUMBER = {01/2001},
  ABSTRACT = {
  The purpose of this article is twofold, first to promote the use of patterns
  in the analysis phase of the software life-cycle by proposing an outline 
  template for analysis patterns that strongly supports the whole analysis 
  process from the requirements analysis to the analysis model and further 
  on to its transformation into a flexible design. Second we present, as 
  an example, a family of analysis patterns that deal with a series of 
  pressing problems in cooperative work, collaborative information filtering 
  and sharing, and knowledge management. We present the step-by-step 
  evolution of the analysis pattern virtual library with active
  agents starting with a  simple pinboard.
  },
  PDF = {http://wwwai.wu-wien.ac.at/~hahsler/research/virlib_working2001/virlib.pdf},
  HTML = {http://wwwai.wu-wien.ac.at/~hahsler/research/virlib_working2001/virlib/virlib.html},
  URL = {http://epub.wu-wien.ac.at/dyn/virlib/wp/showentry?ID=epub-wu-01_a2}
}

@TECHREPORT{hahsler:Hahsler2003,
  AUTHOR = {Michael Hahsler},
  TITLE = {A Quantitative Study of the Application of Design Patterns in Java},
  YEAR = 2003,
  MONTH = JAN,
  ADDRESS = {Augasse 2--6, 1090 Wien, Austria},
  INSTITUTION = {Working Papers on Information Processing and Information Management, Institut f\"ur Informationsverarbeitung und -wirtschaft, Wirschaftsuniversit\"at Wien},
  TYPE = {Working Paper},
  NUMBER = {01/2003},
  ABSTRACT = {
    Using design patterns is a widely accepted method to improve software
    development. There are many benefits of the application of patterns claimed
    in the literature. The most cited claim is that design patterns can provide
    a common design vocabulary and therefore improve greatly communication
    between software designers. Most of the claims are supported by experiences
    reports of practitioners, but there is a lack of quantitative research
    concerning the actual application of design patterns and about the
    realization of the claimed benefits. In this paper we analyze the
    development process of over 1000 open source software projects using
    version control information. We explore this information to gain an insight
    into the differences of software development with and without design
    patterns. By analyzing these differences we provide evidence that design
    patterns are used for communication and that there is a significant
    difference between developers who use design patterns and who do not.
  },
  PDF = {http://wwwai.wu-wien.ac.at/~hahsler/research/patterns_working2003/designpatterns_java.pdf},
  HTML = {http://wwwai.wu-wien.ac.at/~hahsler/research/patterns_working2003/designpatterns_java.html},
  URL = {http://epub.wu-wien.ac.at/dyn/openURL?id=oai:epub.wu-wien.ac.at:epub-wu-01_3f3}
}

@MISC{hahsler:Fessler2003,
  AUTHOR = {Georg Fessler and Michael Hahsler and Michaela Putz
  	and Judith Schwarz and Brigitta Wiebogen},
  TITLE = {{Projektbericht ePubWU 2001-2003}},
  YEAR = 2004,
  MONTH = JAN,
  EDITOR = {},
  ADDRESS = {Augasse 2-6, 1090 Wien},
  INSTITUTION = {Wirschaftsuniversit\"at Wien},
  PAGES = {},
  NOTE = {},
  HOWPUBLISHED = {Augasse 2-6, 1090 Wien, Wirschaftsuniversit\"at Wien},
  ABSTRACT = {ePubWU ist eine elektronische Plattform für wissenschaftliche
  Publikationen der Wirtschaftsuniversität Wien, wo forschungsbezogene
  Veröffentlichungen der WU im Volltext über das WWW zugänglich gemacht werden.
  ePubWU ist seit Jänner 2002 im Echtbetrieb und wird als Gemeinschaftsprojekt
  der Universitätsbibliothek der Wirtschaftsuniversität Wien und der Abteilung
  für Informationswirtschaft betrieben.  Dieser Bericht beinhaltet die
  Erfahrungen aus der 2-jährigen Pilotphase des Projekts.},
  PDF = {http://wwwai.wu-wien.ac.at/~hahsler/research/ePub_bericht_2004/ePub-Projektbericht_01-03.pdf}
}

@TECHREPORT{hahsler:Hafner2004,
  AUTHOR = {Susanne Hafner and Michael Hahsler},
  TITLE = {{Preisvergleich zwischen Online-Shops und traditionellen Geschäften: 	Fallstudie Spieleeinzelhandel}},
  YEAR = 2004,
  MONTH = AUG,
  ADDRESS = {Augasse 2--6, 1090 Wien, Austria},
  INSTITUTION = {Working Papers on Information Processing and Information Management, Institut f\"ur Informationsverarbeitung und -wirtschaft, Wirschaftsuniversit\"at Wien},
  TYPE = {Working Paper},
  NUMBER = {04/2004},
  ABSTRACT = {
  Die vorliegende Arbeit beschäftigt sich mit dem Preisvergleich zwischen
  Online-Shops und traditionellen Geschäften. In einigen Studien wurde bisher
  versucht Preisunterschiede zwischen online und traditionellen Geschäften
  nachzuweisen, um die These, dass Online-Märkte aufgrund höherer Transparenz
  und niedrigerer Transaktionskosten effizienter sind, zu bestätigen. Studien
  untersuchten bisher Produktgruppen wie CDs und Bücher.  In dieser Studie
  beschäftigen wir uns mit dem bisher noch nicht untersuchten
  Spieleeinzelhandel und konzentrieren uns dabei auf den österreichischen
  Markt. Es soll untersucht werden, ob der österreichische Markt ähnliche oder
  andere Ergebnisse liefert als die bisher untersuchten Märkte (hauptsächlich
  im nordamerikanischer Raum).  Die Untersuchung zeigt folgendes: Die Preise
  für Spiele sind im elektronischen Markt um ca. 20 Prozent niedriger als im
  traditionellen Markt.  Die Preisstreuungen im elektronischen und
  traditionellen Markt unterscheiden sich nicht signifikant.  Beide Ergebnisse
  decken sich mit den Ergebnissen anderer Studien. Damit ist der
  österreichische Online-Brettspieleinzelhandel ähnlich entwickelt wie der
  Online-Handel in anderen Ländern und für andere Produktgruppen. 
  },
  PDF = {http://wwwai.wu-wien.ac.at/~hahsler/research/pricing_study_working2004/pricing_study_WP.pdf},
  URL = {http://epub.wu-wien.ac.at/dyn/openURL?id=oai:epub.wu-wien.ac.at:epub-wu-01_75a}
}

@TECHREPORT{hahsler:Hahsler2004c,
  AUTHOR = {Michael Hahsler},
  TITLE = {A Model-Based Frequency Constraint for Mining Associations from
  	Transaction Data},
  YEAR = 2004,
  MONTH = NOV,
  ADDRESS = {Augasse 2--6, 1090 Wien, Austria},
  INSTITUTION = {Working Papers on Information Processing and Information Management, Institut f\"ur Informationsverarbeitung und -wirtschaft, Wirschaftsuniversit\"at Wien},
  TYPE = {Working Paper},
  NUMBER = {07/2004},
  PAGES = {},
  NOTE = {},
  ABSTRACT = {
  In this paper we develop an alternative to minimum support which utilizes
  knowledge of the process which generates transaction data and allows for
  highly skewed frequency distributions. We apply a simple stochastic model
  (the NB model), which is known for its usefulness to describe item
  occurrences in transaction data, to develop a frequency constraint. This
  model-based frequency constraint is used together with a precision threshold
  to find individual support thresholds for groups of associations. We develop
  the notion of NB-frequent itemsets and present two mining algorithms which
  find all NB-frequent itemsets in a database. In experiments with publicly
  available transaction databases we show that the new constraint can provide
  significant improvements over a single minimum support threshold and that the
  precision threshold is easier to use.
  },
  PDF = {http://wwwai.wu-wien.ac.at/~hahsler/research/nbd_working2004/nbd_associationrules_WP.pdf},
  URL = {http://epub.wu-wien.ac.at/dyn/openURL?id=oai:epub.wu-wien.ac.at:epub-wu-01_7a9}
}

@TECHREPORT{hahsler:Hahsler2005b,
  AUTHOR = {Michael Hahsler and Kurt Hornik and Thomas Reutterer},
  TITLE = {Implications of Probabilistic Data
  Modeling for Rule Mining},
  YEAR = 2005,
  MONTH = {March},
  ADDRESS = {Augasse 2--6, 1090 Wien, Austria},
  INSTITUTION = {Research Report Series, Department of Statistics and Mathematics, Wirschaftsuniversit\"at Wien},
  TYPE = {Report},
  NUMBER = 14,
  ABSTRACT = {
  Mining association rules is an important technique for discovering meaningful
  patterns in transaction databases. In the current literature, the properties
  of algorithms to mine associations are discussed in great detail. In this
  paper we investigate properties of transaction data sets from a probabilistic
  point of view. We present a simple probabilistic framework for transaction
  data and its implementation using the R statistical computing environment.
  The framework can be used to simulate transaction data when no associations
  are present. We use such data to explore the ability to filter noise of
  confidence and lift, two popular interest measures used for rule mining.
  Based on the framework we develop the measure hyperlift and we compare this
  new measure to lift using simulated data and a real-world grocery database.
  },
  PDF = {http://wwwai.wu-wien.ac.at/~hahsler/research/probDataMining_wp2005/hyperlift.pdf},
  URL = {http://epub.wu-wien.ac.at/dyn/openURL?id=oai:epub.wu-wien.ac.at:epub-wu-01_7f0}
}

@TECHREPORT{hahsler:Hahsler2005c,
  AUTHOR = {Michael Hahsler and Bettina Gr{\"u}n and Kurt Hornik},
  TITLE = {A Computational Environment for Mining Association Rules and
    Frequent Item Sets},
  YEAR = 2005,
  MONTH = {April},
  ADDRESS = {Augasse 2--6, 1090 Wien, Austria},
  INSTITUTION = {Research Report Series, Department of Statistics and Mathematics, Wirschaftsuniversit\"at Wien},
  TYPE = {Report},
  NUMBER = 15,
  ABSTRACT = {
 Mining frequent itemsets and association rules is a popular and well
   researched approach to discovering interesting relationships between
   variables in large databases.  The R package arules presented
   in this paper provides a basic infrastructure for creating and manipulating
   input data sets and for analyzing the resulting itemsets and rules.  The
   package also includes interfaces to two fast mining algorithms, the popular
   C implementations of Apriori and Eclat by Christian Borgelt.  These
   algorithms can be used to mine frequent itemsets, maximal frequent itemsets,
 closed frequent itemsets and association rules.
  },
  PDF = {http://wwwai.wu-wien.ac.at/~hahsler/research/arules_workingpaper15_2005/arules.pdf},
  URL = {http://epub.wu-wien.ac.at/dyn/openURL?id=oai:epub.wu-wien.ac.at:epub-wu-01_821}
}

@COMMENT{{Command line: /home/users1/csweb/publications_generation_dir/bibtex2html/bib2bib -oc reportkeys -ob publications.bib -c BostonCollegeComputerScience="yes" bibtex/macros.bib bibtex/bibliography.bib bibtex/conferences.bib}}

@Article{thakur:noncontigous,
  author = {Rajeev Thakur and William Gropp and Ewing Lusk},
  title = {Optimizing Noncontiguous Accesses in {MPI-IO}},
  journal = {Parallel Computing},
  year = {2002},
  month = {January},
  volume = {28},
  number = {1},
  pages = {83--105},
  URL = {http://www.mcs.anl.gov/~thakur/papers/mpi-io-noncontig.ps},
  keywords = {parallel I/O, parallel I/O, MPI-IO, collective I/O, data sieving,
  pario-bib},
  abstract = {The I/O access patterns of many parallel applications consist of
  accesses to a large number of small, noncontiguous pieces of data. If an
  application's I/O needs are met by making many small, distinct I/O requests,
  however, the I/O performance degrades drastically. To avoid this problem,
  MPI-IO allows users to access noncontiguous data with a single I/O function
  call, unlike in Unix I/O. In this paper, we explain how critical this feature
  of MPI-IO is for high performance and how it enables implementations to
  perform optimizations. We first provide a classification of the different
  ways of expressing an application's I/O needs in MPI-IO---we classify them
  into four {\em levels}, called level~0 through level~3. We demonstrate that,
  for applications with noncontiguous access patterns, the I/O performance
  improves dramatically if users write their applications to make level-3
  requests (noncontiguous, collective) rather than level-0 requests (Unix
  style). We then describe how our MPI-IO implementation, ROMIO, delivers high
  performance for noncontiguous requests. We explain in detail the two key
  optimizations ROMIO performs: data sieving for noncontiguous requests from
  one process and collective I/O for noncontiguous requests from multiple
  processes. We describe how we have implemented these optimizations portably
  on multiple machines and file systems, controlled their memory requirements,
  and also achieved high performance. We demonstrate the performance and
  portability with performance results for three applications---an
  astrophysics-application template (DIST3D), the NAS BTIO benchmark, and an
  unstructured code (UNSTRUC)---on five different parallel machines: HP
  Exemplar, IBM SP, Intel Paragon, NEC SX-4, and SGI Origin2000.}
}

@InProceedings{reddy:compiler,
  author = {A. L. Narasimha Reddy and P. Banerjee and D. K. Chen},
  title = {Compiler Support for Parallel {I/O} Operations},
  booktitle = {Proceedings of the 1991 International Conference on Parallel
  Processing},
  year = {1991},
  pages = {II:290--II:291},
  publisher = {CRC Press},
  address = {St. Charles, IL},
  earlier = {reddy:compiler-tr},
  keywords = {parallel I/O, pario-bib, compilers},
  comment = {This version is only 2 pages. reddy:compiler-tr provides the full
  text. They discuss three primary issues. 1) Overlapping I/O with computation:
  the compiler's dependency analysis is used to decide when some I/O may be
  moved up and performed asynchronously with other computation. 2) Parallel
  execution of I/O statements: {\em if} all sizes are known at compile time,
  the compiler can insert seeks so that processes can access the file
  independently. When writing in the presence of conditionals they even propose
  skipping by the maximum and leaving holes in the file, and they claim that
  this doesn't hurt (!). 3) Parallel format conversion: again, if there are
  fixed-width fields the compiler can have processors seek to different
  locations, read data independently, and do format conversion in parallel.
  Really all this is saying is that fixed-width fields are good for
  parallelism, and that compilers could take advantage of them.}
}

@TECHREPORT{MSU-CSE-99-39,
  AUTHOR =        {Lin Hong and Anil K. Jain and Sharath Pankanti},
  TITLE =         {Can multibiometrics improve performance},
  NUMBER =        {MSU-CSE-99-39},
  INSTITUTION =   {Department of Computer Science, Michigan State University},
  ADDRESS =       {East Lansing, Michigan},
  ABSTRACT =      {While it is widely acknowledged that the performance
                   improvement in current biometrics-based personal
                   authentication systems is necessary, it is not clear what
                   mechanisms could be used to improve the performance. In this
                   paper, we formulate the problem of multiple biometrics
                   integration and examine whether the improvement in
                   performance could be achieved from integrating multiple
                   biometrics. For two practical and commonly used situations
                   of multibiometric integration, we analyze the performance
                   gains. We also demontrate empirically that integration of
                   multiple biometrics does indeed result in consistent and
                   significant performance improvement.},
  KEYWORDS =      {},
  NOTE =          {},
  MONTH =         {December},
  YEAR  =         {1999},
  AUTHOR1_URL =   {http://www.cse.msu.edu/~jain},
  AUTHOR1_EMAIL = {jain@cse.msu.edu},
  AUTHOR2_URL =   {http://www.cse.msu.edu/~honglin},
  AUTHOR2_EMAIL = {honglin@cse.msu.edu},
  PAGES =         {6},
  FILE  =         {/user/web/htdocs/publications/tech/TR/MSU-CSE-99-39.ps},
  URL   =         {},
  CONTACT =       {jain@cse.msu.edu}
}

@COMMENT{{This file has been generated by bib2bib 1.69}}

@COMMENT{{Command line: C:\cygwin\bin\bib2bib-1.69.exe -oc conf -ob conf.bib -c '$type = "INPROCEEDINGS"' cormode.bib}}

@INPROCEEDINGS{CormodeMuthukrishnan05PODS,
  BOOKTITLE = {Proceedings of {ACM} Principles of Database Systems},
  AUTHOR = {G. Cormode and S. Muthukrishnan},
  TITLE = {Space Efficient Mining of Multigraph Streams},
  YEAR = {2005},
  ABSTRACT = {
  The challenge of monitoring massive amounts of data generated by
  communication networks has led to the interest in data stream
  processing. 
  We study streams of edges in massive communication multigraphs, defined by
  (source, destination) pairs. 
  The goal is to compute properties of the underlying graph while using small
  space (much smaller than the number of communicants), 
  and to avoid bias introduced because some edges may appear many times,
  while others are seen only once. 
  We give results for three fundamental problems on multigraph degree sequences: 
  estimating frequency moments of degrees, finding the heavy hitter
  degrees, and computing range sums of degree values. 
  In all cases we are able to show space bounds
  for our summarizing algorithms
  that are significantly smaller than storing complete information.
  We use a variety of data stream methods: 
  sketches, sampling, hashing and distinct
  counting, but a common feature is that we use the novel approach of
  {\em cascaded summaries}: nesting multiple estimation techniques within one
  another. 
  In our experimental study, we see that such summaries are highly
  effective, enabling massive multigraph streams to be effectively
  summarized to answer queries of interest with high accuracy using only
  a small amount of space. }
}

@INPROCEEDINGS{CormodeGarofalakisMuthukrishnan05SIGMOD,
  BOOKTITLE = {Proceedings of {ACM} {SIGMOD} International 
                    Conference on Management of Data},
  AUTHOR = {G. Cormode and M. Garofalakis and S. Muthukrishnan},
  TITLE = {Holistic Aggregates in a Networked World: 
                   Distributed Tracking of Approximate Quantiles},
  YEAR = {2005},
  ABSTRACT = {
  While traditional database systems optimize for performance on
  one-shot queries, emerging large-scale monitoring applications
  require continuous tracking of  complex aggregates  and
  data-distribution summaries over collections of
  physically-distributed streams.
  Thus, effective solutions have to be simultaneously space efficient
  (at each remote site), communication efficient (across the underlying
  communication network), and provide continuous, guaranteed-quality
  estimates.
  In this paper, we propose novel algorithmic solutions
  for the problem of continuously tracking complex holistic
  aggregates in such a distributed-streams setting ---
  our primary focus is on approximate quantile summaries, but our
  approach is more broadly applicable and can handle other
  holistic-aggregate functions (e.g., ``heavy-hitters'' queries).
  We present the first known distributed-tracking schemes  for
  maintaining accurate quantile estimates  with provable
  approximation guarantees, while simultaneously optimizing
the storage space at each remote site as well as the
communication cost across the network.
In a nutshell, our algorithms employ a combination
of local tracking at remote sites and simple prediction
models for local site behavior in order to produce highly
communication- and space-efficient solutions.
We perform extensive experiments with real and synthetic data
to explore the various tradeoffs and understand the role of
prediction models in our schemes.
The results clearly validate our approach, revealing significant
savings over naive solutions as well as our analytical worst-case
guarantees.}
}

@INPROCEEDINGS{CormodeMuthukrishnan05CM,
  BOOKTITLE = {{SIAM} Conference on Data Mining},
  AUTHOR = {G. Cormode and S. Muthukrishnan},
  TITLE = {Summarizing and Mining Skewed Data Streams},
  YEAR = {2005},
  URL = {../cmz-sdm.pdf},
  ABSTRACT = {
Many applications generate massive data streams.
Summarizing such massive data requires fast, small space algorithms to
support post-hoc queries and mining. 
An important observation is that such streams are rarely uniform, and
real data sources typically exhibit significant skewness. 
These are well modeled by Zipf distributions, which are characterized
by a parameter, $z$, that captures the amount of skew.

We present a data stream summary that can answer point queries with 
$eps$ accuracy and show that the space needed is only
$O(eps^{-\min\{1,1/z\}})$. 
This is the first $o(1/eps)$ space algorithm for this problem,
and we show it is essentially tight for skewed distributions. 
We show that the same data structure can also estimate the $L_2$ norm
of the stream in $o(1/eps^2)$ space for $z>1/2$, another
improvement over the existing $\Omega(1/eps^2)$ methods. 

We support our theoretical results with an experimental study over a
large variety of real and synthetic data. 
We show that significant skew is present in both textual and
telecommunication data.
Our methods give strong accuracy, significantly better than other
methods, and behave exactly in line with their analytic bounds.}
}

@INPROCEEDINGS{CormodeKornMuthukrishnanSrivastava05,
  AUTHOR = {G. Cormode and F. Korn and S. Muthukrishnan and D. 
                  Srivastava},
  TITLE = {Effective Computation of Biased Quantiles over Data 
                    Streams},
  YEAR = {2005},
  BOOKTITLE = {21st International Conference on Data Engineering 
                   (ICDE)},
  URL = {../bquant.pdf},
  ABSTRACT = {
Skew is prevalent in many data sources such as IP traffic
streams.  To continually summarize the 
distribution of such data, a high-biased set of quantiles
(e.g., 50th, 90th and 99th percentiles) with finer error
guarantees at higher ranks (e.g., errors of 5, 1 and 0.1
percent, respectively) is more useful than uniformly 
distributed quantiles (e.g., 25th, 50th and 75th percentiles)
with uniform error guarantees.
In this paper, we address the following two problems.  
First, can we compute quantiles with finer 
error guarantees for the higher ranks of the data distribution 
effectively, using less space and computation time than 
computing all quantiles uniformly at the finest error?
Second, if {\em specific\/} quantiles and their error bounds 
are requested {\em a priori}, can the necessary space usage and 
computation time be reduced?

We answer both questions in the affirmative by formalizing them as the
``high-biased'' quantiles and the ``targeted'' quantiles problems,
respectively, and presenting algorithms with provable guarantees, that 
perform
significantly better than previously known solutions for these problems.  
We implemented our algorithms in the Gigascope data stream management
system, and evaluated alternate approaches for maintaining the
relevant summary structures.  Our experimental results on real and
synthetic IP data streams complement our theoretical analyses, and
highlight the importance of lightweight, non-blocking implementations
when maintaining summary structures over high-speed data streams.}
}

@INPROCEEDINGS{CormodeMuthukrishnan05,
  AUTHOR = {G. Cormode and S. Muthukrishnan},
  TITLE = {Substring Compression Problems},
  YEAR = {2005},
  BOOKTITLE = {{ACM-SIAM} Symposium on Discrete 
                   Algorithms},
  URL = {../substringsoda.pdf},
  SLIDES = {../substring-slides.pdf},
  ABSTRACT = {
We initiate a new class of string matching problems called 
{\em  Substring Compression Problems}.
Given a string $S$ that may be preprocessed, 
the problem is to quickly find the
compressed representation or the compressed size of any query substring of 
$S$ 
(Substring Compression Query or SCQ) or to find the length $\ell$ 
substring of $S$ 
whose compressed size is the least (Least Compressible Substring or LCS 
problem). 

Starting from the seminal paper of Lempel and Ziv over 25 years ago,
many different methods have emerged for compressing entire strings.
Determining substring compressibility is a natural variant that is
combinatorially and algorithmically challenging, yet surprisingly
has not been studied before. In addition, compressibility of strings
is emerging as a tool to compare biological sequences and analyze their
information content. However, typically, the compressibility 
of the entire sequence is not as informative as that of portions of
the sequences. Thus substring compressibility may be a more suitable 
basis for sequence analysis.

We present the first known, nearly optimal algorithms for substring 
compression problems---SCQ, LCS and their generaliztions---that 
are exact or provably approximate. 
Our exact algorithms exploit the structure in strings via suffix trees  
and 
our approximate algorithms rely on new relationships we find between 
Lempel-Ziv compression and string parsings. }
}

@INPROCEEDINGS{CormodeKornMutukrishnanSrivastava04,
  AUTHOR = {G. Cormode and F. Korn and S. Muthukrishnan and D. Srivastava},
  YEAR = {2004},
  TITLE = {Diamond in the Rough: Finding Hierarchical Heavy Hitters in Multi-Dimensional Data},
  BOOKTITLE = {Proceedings of SIGMOD},
  PAGES = {155--166},
  URL = {../ckmsh4.pdf},
  ABSTRACT = {
  Data items archived in data warehouses or those that arrive online as
streams typically have attributes which take values from multiple
hierarchies (e.g., time and geographic location; source and
destination IP addresses). Providing an aggregate view of such data is
important to summarize, visualize, and analyze. We develop the
aggregate view based on certain hierarchically organized sets of
large-valued regions (``heavy hitters''). Such Hierarchical Heavy
Hitters (HHHs) were previously introduced as a crucial aggregation
technique in one dimension. In order to analyze the wider range of
data warehousing applications and realistic IP data streams, we
generalize this problem to multiple dimensions.

We illustrate and study two variants of HHHs for multi-dimensional
data. In particular, we identify ``overlap'' and ``split'' variants,
depending on how an aggregate computed for a child node in the
multi-dimensional hierarchy is propagated to its parent
element(s). For data warehousing applications, we present offline
algorithms that take multiple passes over the data and produce the
exact HHHs.  For data stream applications, we present online
algorithms that find approximate HHHs in one pass, with proven
accuracy guarantees.

We show experimentally, using real and synthetic data, that our
proposed online algorithms yield outputs which are very similar
(virtually identical, in many cases) to their offline
counterparts. The lattice property of the product of hierarchical
dimensions (``diamond'') is crucially exploited in our online
algorithms to track approximate HHHs using only a small, fixed number
of statistics per candidate node, regardless of the number of
dimensions.}
}

@INPROCEEDINGS{CormodeKornMuthukrishnanJohnsonSpatsckechSrivastava04,
  AUTHOR = {G. Cormode and F. Korn and S. Muthukrishnan and T. Johnson and O. Spatscheck and D. Srivastava},
  YEAR = {2004},
  TITLE = {Holistic UDAFs at streaming speeds},
  PAGES = {35--46},
  URL = {../cjkmss-streamudaf.pdf},
  BOOKTITLE = {Proceedings of SIGMOD},
  ABSTRACT = {
Many algorithms have been proposed to approximate holistic ag-
gregates, such as quantiles and heavy hitters, over data streams.
However, little work has been done to explore what techniques are
required to incorporate these algorithms in a data stream query pro-
cessor, and to make them useful in practice.
In this paper, we study the performance implications of using
user-dened aggregate functions (UDAFs) to incorporate selection-
based and sketch-based algorithms for holistic aggregates into a
data stream management system's query processing architecture.
We identify key performance bottlenecks and tradeoffs, and pro-
pose novel techniques to make these holistic UDAFs fast and space-
efcient for use in high-speed data stream applications. We evalu-
ate performance using generated and actual IP packet data, focus-
ing on approximating quantiles and heavy hitters. The best of our
current implementations can process streaming queries at OC48
speeds (2x 2.4Gbps).}
}

@INPROCEEDINGS{Cormode04,
  AUTHOR = {G. Cormode},
  TITLE = {The Hardness of the Lemmings Game, or {Oh} no, more {NP}-Completeness Proofs},
  URL = {../cormodelemmings.pdf},
  LINK = {Cormode04LemsTR.html},
  PAGES = {65--76},
  BOOKTITLE = {Proceedings of Third International Conference on Fun with Algorithms},
  YEAR = {2004},
  ABSTRACT = {
In the computer game `Lemmings', the player must guide a tribe of green-haired
lemming creatures to safety, and save them from an untimely demise. 
We formulate the decision problem which is, given a level of the game, to
decide whether it is possible to complete the level (and hence to find 
a solution to that level).  
Under certain limitations, this can be decided in polynomial
time, but in general the problem is shown to be NP-Hard.  
This can hold even if there
is only a single lemming to save, thus showing that it is hard to
approximate the number of saved lemmings to any factor.}
}

@INPROCEEDINGS{CormodeCzumajMuthukrishnan04,
  AUTHOR = {G. Cormode and A. Czumaj and S. Muthukrishnan},
  TITLE = {How to {\em increase} the acceptance ratios of top conferences},
  URL = {http://www.cs.rutgers.edu/~muthu/ccmfun.pdf},
  BOOKTITLE = {Proceedings of Third International Conference on Fun with Algorithms},
  YEAR = {2004},
  PAGES = {262--273},
  LINK = {CormodeCzumajMuthukrishnan04TR.html},
  ABSTRACT = {
 In the beginning was the pub. This work was triggered by a pub conversation where the
authors observed that many resumes list acceptance ratios of conferences where their
papers appear, boasting the low acceptance ratio. The lower the ratio, better your paper
looks. The list might look equally impressive if one listed the rejection ratio of
conferences where ones paper was submitted and rejected. We decided to lampoon
rather than lament the effort the PC typically put in: wouldn't the world be better if we
could encourage only high quality submissions and so run top conferences with very
high acceptance ratios? This paper captures our thoughts, and it is best consumed in a
pub (and in color).}
}

@INPROCEEDINGS{CormodeMuthukrishnan04Infocom,
  AUTHOR = {G. Cormode and S. Muthukrishnan},
  TITLE = {What's New: Finding Significant Differences in Network
                     Data Streams},
  BOOKTITLE = {Proceedings of {IEEE} {Infocom}},
  YEAR = {2004},
  PAGES = {1534--1545},
  URL = {../whatsnew.pdf},
  SLIDES = {../changes-infocom.pdf},
  ABSTRACT = {Monitoring and analyzing network traffic usage patterns 
is vital for managing IP Networks. An important problem is to provide 
network managers with information about changes in traffic, informing them 
about ``what's new''. Specifically, we focus on the challenge of finding 
significantly large differences in traffic: over time, between interfaces 
and between routers. We introduce the idea of a deltoid: an item that has 
a large difference, whether the difference is absolute, relative or 
variational.

We present novel algorithms for finding the most significant deltoids in 
high speed traffic data, and prove that they use small space, very small 
time per update, and are guaranteed to find significant deltoids with 
pre-specified accuracy. In experimental evaluation with real network 
traffic, our algorithms perform well and recover almost all deltoids. This 
is the first work to provide solutions capable of working over the data 
with one pass, at network traffic speeds.}
}

@INPROCEEDINGS{CormodeMuthukrishnan04CMLatin,
  AUTHOR = {G. Cormode and S. Muthukrishnan},
  TITLE = {An Improved Data Stream Summary: The Count-Min Sketch 
                     and its Applications},
  YEAR = {2004},
  BOOKTITLE = {Proceedings of Latin American Theoretical Informatics (LATIN)},
  PAGES = {29-38},
  URL = {../cm-latin.pdf},
  SLIDES = {../cmlatintalk.pdf},
  LINK = {CormodeMuthukrishnan04CMJalg.html},
  LINK2 = {http://springerlink.metapress.com/openurl.asp?genre=article&issn=0302-9743&volume=2976&spage=29},
  ABSTRACT = {We introduce a new sublinear space data structure---the Count-Min Sketch--- for summarizing data streams. Our sketch allows fundamental queries in data stream summarization such as point, range, and inner product queries to be approximately answered very quickly; in addition, it can be applied to solve several important problems in data streams such as finding quantiles, frequent items, etc. The time and space bounds we show for using the CM sketch to solve these problems significantly improve those previously known --- typically from 1/eps^2 to 1/eps in factor.

Journal version appeared in Journal of Algorithms.
}
}

@INPROCEEDINGS{Cormode03Stable,
  AUTHOR = {G. Cormode},
  TITLE = {Stable distributions for stream computations: 
                    it's as easy as 0,1,2},
  BOOKTITLE = {Workshop on Management and Processing of Massive Data
                     Streams at FCRC},
  YEAR = {2003},
  URL = {../stable.ps},
  SLIDES = {../mpds-stable.pdf},
  ABSTRACT = {A surprising number of data stream problems are solved by methods involving computations with stable distributions. This paper will give a short summary of some of these problems, and how the best known solutions depend on use of stable distributions; it also lists some related open problems.}
}

@INPROCEEDINGS{CormodeMuthukrishnan03PODS,
  TITLE = {What's Hot and What's Not: Tracking Most Frequent Items
                Dynamically},
  YEAR = {2003},
  PAGES = {296-306},
  AUTHOR = {G. Cormode and S. Muthukrishnan},
  BOOKTITLE = {Proceedings of {ACM} Principles of Database Systems},
  URL = {../hotitems.pdf},
  SLIDES = {../whatshot-pods.pdf},
  LINK2 = {http://doi.acm.org/10.1145/773153.773182},
  ABSTRACT = {Most database management systems maintain statistics on the underlying relation. One of the important statistics is that of the 'hot items' in the relation: those that appear many times (most frequently, or more than some threshold). For example, end-biased histograms keep the hot items as part of the histogram and are used in selectivity estimation. Hot items are used as simple outliers in data mining, and in anomaly detection in networking applications. We present a new algorithm for dynamically determining the hot items at any time in the relation that is undergoing deletion operations as well as inserts. Our algorithm maintains a small space data structure that monitors the transactions on the relation, and when required, quickly outputs all hot items, without rescanning the relation in the database. With user-specified probability, it is able to report all hot items. Our algorithm relies on the idea of ``group testing'', is simple to implement, and has provable quality, space and time guarantees. Previously known algorithms for this problem that make similar quality and performance guarantees can not handle deletions, and those that handle deletions can not make similar guarantees without rescanning the database. Our experiments with real and synthetic data shows that our algorithm is remarkably accurate in dynamically tracking the hot items independent of the rate of insertions and deletions.}
}

@INPROCEEDINGS{CormodeKornMuthukrishnanSrivastava03,
  TITLE = {Finding Hierarchical Heavy Hitters in Data Streams},
  YEAR = {2003},
  PAGES = {464-475},
  AUTHOR = {G. Cormode and F. Korn and S. Muthukrishnan and
D. Srivastava},
  BOOKTITLE = {International Conference on Very Large Databases},
  URL = {../ckms-hhh.pdf},
  LINK = {http://www.vldb.org/conf/2003/papers/S15P01.pdf},
  ABSTRACT = {Aggregation along hierarchies is a critical summary technique in a large variety of online applications including decision support, and network management (e.g., IP clustering, denial-of-service attack monitoring). Despite the amount of recent study that has been dedicated to online aggregation on sets (e.g., quantiles, hot items), surprisingly little attention has been paid to summarizing hierarchical structure in stream data.

The problem we study in this paper is that of finding Hierarchical Heavy Hitters (HHH): given a hierarchy and a fraction phi, we want to find all HHH nodes that have a total number of descendants in the data stream larger than phi of the total number of elements in the data stream, after discounting the descendant nodes that are HHH nodes. The resulting summary gives a topological 'cartogram' of the hierarchical data. We present deterministic and randomized algorithms for finding HHHs, which builds upon existing techniques by incorporating the hierarchy into the algorithms. Our experiments demonstrate several factors of improvement in accuracy over the straightforward approach, which is due to making algorithms hierarchy-aware}
}

@INPROCEEDINGS{CormodeMuthukrishnan03DN,
  AUTHOR = {G. Cormode and S. Muthukrishnan},
  YEAR = {2003},
  BOOKTITLE = {Proceedings of the 11th European Symposium on Algorithms 
                    (ESA)},
  TITLE = {Estimating Dominance Norms of Multiple Data Streams},
  SERIES = {LNCS},
  VOLUME = {2838},
  URL = {../dominance.pdf},
  SLIDES = {../esadom.pdf},
  LINK = {CormodeMuthukrishnan02DomTR.html},
  LINK2 = {http://springerlink.metapress.com/openurl.asp?genre=article&issn=0302-9743&volume=2832&spage=148},
  ABSTRACT = {
There is much focus in the algorithms and database communities on
designing tools to manage and mine data streams.
Typically, data streams consist of multiple signals.
Formally, a stream of multiple signals is $(i,a_{i,j})$
where $i$'s correspond to the domain, $j$'s index the different
signals and $a_{i,j}\geq 0$ give the value of the $j$th signal at
point $i$.
We study the problem of finding norms that are cumulative of
the multiple signals in the data stream.

For example, consider the max-dominance norm,
defined as $\Sigma_i \max_j \{a_{i,j}\}$.
It may be thought as estimating the norm of the  ``upper
envelope'' of the multiple signals, or alternatively, as
estimating the norm of the ``marginal'' distribution of tabular data streams.
It is used in applications to estimate the ``worst case influence'' of
multiple processes, for example in IP traffic analysis, electrical
grid monitoring and
financial domain.
In addition, it is a natural measure, generalizing the union of data
streams or counting distinct elements in data streams.

We present the first known data stream algorithms for estimating
max-dominance of multiple signals.
In particular, we use workspace and time-per-item that are both sublinear (in
fact, poly-logarithmic)  in the input size.  In contrast
other
notions of dominance on streams $a$, $b$ --- min-dominance
($\Sigma_i \min_j \{a_{i,j}\}$),
count-dominance ($|\{i | a_i > b_i\}|$) or relative-dominance
($\Sigma_i a_i/\max\{1,b_i\}$ ) --- are all impossible to estimate
accurately with sublinear space.}
}

@INPROCEEDINGS{CormodeDatarIndykMuthukrishnan02,
  AUTHOR = {G. Cormode and M. Datar and P. Indyk and 
                   S. Muthukrishnan},
  TITLE = {Comparing Data Streams Using {H}amming Norms},
  YEAR = {2002},
  PAGES = {335--345},
  BOOKTITLE = {Proceedings of 28th International Conference on Very Large 
                 Data Bases},
  URL = {../hammingnorm.pdf},
  SLIDES = {../vldb02.pdf},
  LINK = {CormodeDatarIndykMuthukrishnan03.html},
  LINK2 = {http://www.vldb.org/conf/2002/S10P02.pdf},
  ABSTRACT = {
Massive data streams are now fundamental to many data processing
applications.  For example, Internet routers produce large scale
diagnostic data streams.  Such streams are rarely stored in
traditional databases, and instead must be processed ``on the fly'' as
they are produced.  Similarly, sensor networks produce multiple data
streams of observations from their sensors.  There is growing focus on
manipulating  data streams, and hence, there is a need to identify
basic operations of interest in managing data streams, and to support
them efficiently.
We propose computation of the Hamming norm as a basic operation of
interest.  The Hamming norm formalizes ideas
that are used throughout data processing.
When applied to a single stream, the Hamming norm gives the number of distinct 
items that are
present in that data stream, which is a statistic of great interest in
databases.  When applied to a pair of streams, the Hamming norm gives
an important measure of (dis)similarity:  the number of unequal item
counts in the two streams. Hamming norms have many uses
in comparing data streams.

We present a novel approximation technique for estimating the Hamming norm
for massive data streams; this relies on what we call the ``$l_0$
{\it sketch}''
and we prove its accuracy. We test our approximation method on a large
quantity of synthetic and real stream data, and show that the
estimation is accurate to within a few percentage points. 

  Journal version 
                 in {\em {IEEE} Transactions on Knowledge and Data
                 Engineering} 15(3):529--541, 2003}
}

@INPROCEEDINGS{CormodeIndykKoudasMuthukrishnan02,
  AUTHOR = {G. Cormode and P. Indyk and N. Koudas and 
                  S. Muthukrishnan},
  TITLE = {Fast Mining of Tabular Data via 
                  Approximate Distance Computations},
  YEAR = {2002},
  PAGES = {605--616},
  BOOKTITLE = {Proceedings of the International Conference on Data 
                  Engineering},
  URL = {../tabular.pdf},
  SLIDES = {../icdeslides.pdf},
  LINK2 = {http://csdl.computer.org/comp/proceedings/icde/2002/1531/00/15310605abs.htm},
  ABSTRACT = {
Tabular data abound in many  data  stores: traditional
relational databases store tables, and
new applications also generate massive tabular
datasets.
For example, consider the geographic distribution of cell
phone traffic at different  base stations across the
country
(which can be  represented by a table indexed by
  latitude and longitude),
or the evolution of traffic at Internet
routers over time
(which can be represented by a table
indexed by time, source and destination IP addresses).
Detecting similarity patterns in such data
sets (e.g., which geographic regions have similar cell phone
usage  distribution,  which  IP subnet traffic distributions
over time intervals are similar,  etc) is of great importance.
Identification   of  such  patterns  poses  many
conceptual challenges (what is a  suitable  similarity
distance  function  for two ``regions'') as well as
technical challenges (how to perform similarity computations
efficiently as massive tables get accumulated over time) that
we address.

We  present methods  for  determining
similar regions in massive tabular data.
Our methods are for computing the ``distance'' between
any two subregions of a tabular data: they are approximate,
but  highly  accurate as we prove mathematically, and they are
fast, running in time nearly linear in the table size. Our methods
are general since these distance computations can be
applied to any  mining or
similarity   algorithms that use $L_p$ norms.
A novelty of our distance computation procedures is that they
work for any $L_p$ norms --- not only the traditional
$p=2$  or  $p=1$,  but for all $p\leq 2$; the choice of $p$,
say {\em fractional} $p$,
provides an interesting alternative similarity behavior!
We use  our algorithms in  a  detailed
experimental study of the clustering patterns in  real  tabular
data obtained  from  one  of  AT\&T's  data stores and show that our
methods  are  substantially  faster   than   straightforward
methods  while remaining highly accurate, and able to detect
interesting patterns by varying the value of $p$.}
}

@INPROCEEDINGS{CormodeMuthukrishnan02,
  AUTHOR = {G. Cormode and S. Muthukrishnan},
  TITLE = {The String Edit Distance Matching Problem with Moves},
  YEAR = {2002},
  BOOKTITLE = {Proceedings of the 13th Annual Symposium on Discrete
                  Algorithms},
  PAGES = {667--676},
  URL = {../editmoves.pdf},
  SLIDES = {../soda2k2.pdf},
  LINK = {CormodeMuthukrishnan01EditTR.html},
  LINK2 = {http://doi.acm.org/10.1145/545381.545470},
  ABSTRACT = {
The edit distance between two strings $S$ and $R$
is defined to be the minimum number of character inserts,
deletes and changes needed to convert $R$ to $S$.
Given a text string $t$ of length $n$, and a pattern string $p$
of length $m$, informally, the string edit distance matching problem is to
compute the smallest edit distance between $p$ and
substrings of $t$. A well known dynamic programming algorithm
takes time $O(nm)$ to solve this problem, and it is an important
open problem in Combinatorial Pattern Matching to
significantly improve this bound.

We relax the problem
so that (a) we allow an additional operation, namely,
{\em substring moves}, and (b) we approximate the string edit distance
upto a factor of $O(\log n \log^* n)$.\footnote{$\log^* n$ is the
number of times $\log$ function is applied to $n$ to produce a constant.}
Our result is a near linear time deterministic
algorithm for this version of the problem.
This is the first
known significantly subquadratic algorithm for a string
edit distance problem in which the distance involves
nontrivial alignments.
Our results are obtained by embedding strings into $L_1$
vector space using a simplified parsing technique we call
{\em Edit Sensitive Parsing} (ESP). This embedding is
approximately distance preserving, and we show many applications
of this embedding to string proximity problems including nearest neighbors,
outliers, and streaming computations with strings.
}
}

@INPROCEEDINGS{CormodeMuthukrishnanSahinalp01,
  AUTHOR = {G. Cormode and S Muthukrishnan 
                   and S. C. {{S}}ahinalp},
  TITLE = {Permutation Editing and Matching via Embeddings},
  BOOKTITLE = {Proceedings of 28th International Colloquium on Automata, 
                  Languages and Programming},
  VOLUME = {2076},
  YEAR = {2001},
  PAGES = {481--492},
  URL = {../cms-perm.pdf},
  SLIDES = {../icalp.pdf},
  LINK2 = {http://link.springer.de/link/service/series/0558/bibs/2076/20760481.htm},
  ABSTRACT = {
If the genetic maps of two species are modelled as permutations of
 (homologous) genes, the number of chromosomal rearrangements
 in the form of deletions, block moves, inversions etc. to transform one
 such permutation to another can be used as a measure of their evolutionary
 distance. Motivated by such scenarios in Computational Biology and
elsewhere, we study problems of
computing distances between permutations
 as well as matching permutations in
 sequences, and finding most similar permutation from a collection
 (``nearest neighbor'').

We adopt a
 general approach: embed permutation distances of relevance into well-known
 vector spaces in an approximately distance-preserving manner,
 and solve the resulting problems on the well-known spaces.
Our results are as follows:
\begin{itemize}
\item
We present the first known approximately distance preserving
embeddings of these permutation distances into well-known spaces.
\item
Using these embeddings, we obtain several results, including
the first known efficient
 solution for approximately solving nearest neighbor
 problems with permutations
 and
the first known
 algorithms for finding permutation distances in the ``data stream''
 model.
\item
We consider a novel
 class of problems called {\em permutation matching} problems
which are similar to string matching problems,
except that the
pattern is a permutation (rather than a string)
and present linear or near-linear time
algorithms for approximately solving permutation matching problems;
in contrast, the corresponding string problems take significantly
longer.
}
}

@INPROCEEDINGS{CormodePatersonSahinalpVishkin00,
  AUTHOR = {G. Cormode and M. Paterson and S. C.
                   {{S}}ahinalp and U. Vishkin},
  TITLE = {Communication Complexity of Document Exchange},
  BOOKTITLE = {Proceedings of the 11th Symposium on Discrete
                  Algorithms},
  YEAR = {2000},
  PAGES = {197--206},
  URL = {../docexchange.pdf},
  LINK = {http://doi.acm.org/10.1145/338219.338252},
  SLIDES = {../soda.pdf},
  ABSTRACT = {
We have two users, $A$ and $B$, who hold documents $x$ and $y$ respectively.
Neither of the users has any information about the other's document.
They exchange messages
 so that $B$ computes $x$; it may be required that $A$ compute
 $y$ as well.
Our goal is to design communication protocols with
 the main objective of minimizing the total number of bits they exchange;
 other objectives are
 minimizing the number of rounds and the complexity of internal
 computations.
An important notion which determines the efficiency of the
 protocols is how one measures the distance between $x$ and $y$.
We consider several metrics for measuring this distance,
 namely the Hamming metric, the Levenshtein metric (edit distance),
 and a new LZ metric, which is introduced in this paper.
We show how to estimate the distance between $x$ and $y$ using a
 single message of logarithmic size.
For each metric, we present the first
 communication-efficient protocols, which often
 match the corresponding lower bounds.
A consequence of these are error-correcting codes for these error
 models which correct up to $d$ errors in $n$ characters using
 $O(d \log n)$ bits.
Our most interesting methods use a new
 {\em histogram transformation} that we introduce to convert
 edit distance to $L_1$ distance.}
}

@INPROCEEDINGS{OzsoyogluCormodeBalkirOzsoyoglu00,
  AUTHOR = {G. Ozsoyoglu and N. H. Balkir and
                  G. Cormode and  Z. M. Ozsoyoglu},
  TITLE = {Electronic Books in Digital Libraries},
  YEAR = {2000},
  BOOKTITLE = {Proceedings of IEEE Advances in Digital Libraries (ADL)},
  PAGES = {5--14},
  URL = {../electronicbooks.pdf},
  LINK = {OzsoyogluCormodeBalkirOzsoyoglu04.html},
  LINK2 = {http://csdl.computer.org/comp/proceedings/adl/2000/0659/00/0659toc.htm},
  ABSTRACT = {
Electronic Book is an application with a multimedia database of instructional resources, which include hyperlinked text, instructor's audio/video clips, slides, animation, still images, etc. as well as content-based infomration about these data, and metadata such as annotations, tags, and cross-referencing information. Electronic books in the Internet or on CDs today are not easy to learn from. We propose the use of a multimedia database of instructional resources in constructing and delivering multimedia lessons about topics in an electronic book.

We introduce an electronic book data model containing (a) topic objects and and (b) instructional resources, called instruction modules which are multimedia presentations possibly capturing real-life lectures of instructors. We use the notion of topic prerequisites for topics at different detail levels, to allow electronic book users to request / compose multimedia lessons about topics on the electronic book. We present automated construction of the ''best'' user-tailored lesson (as a multimedia presentation). }
}

@COMMENT{{Command line: /srv/ai/localhost/home/staff/hahsler/bin/bib2bib -c 'not $type = "INPROCEEDINGS" & not $type = "INCOLLECTION" & not $type = "ARTICLE"' -ob misc.bib hahsler.bib}}

@INPROCEEDINGS{vepa-king-taylor_icslp02,
  AUTHOR = {Vepa, J. and King, S. and Taylor, P.},
  TITLE = {Objective Distance Measures for Spectral
                   Discontinuities in Concatenative Speech Synthesis},
  BOOKTITLE = {Proc. {ICSLP}},
  ADDRESS = {Denver, USA},
  ABSTRACT = {In unit selection based concatenative speech systems,
                   `join cost', which measures how well two units can be
                   joined together, is one of the main criteria for
                   selecting appropriate units from the inventory. The
                   ideal join cost will measure `perceived' discontinuity,
                   based on easily measurable spectral properties of the
                   units being joined, in order to ensure smooth and
                   natural-sounding synthetic speech. In this paper we
                   report a perceptual experiment conducted to measure the
                   correlation between `subjective' human perception and
                   various `objective' spectrally-based measures proposed
                   in the literature. Our experiments used a
                   state-of-the-art unit-selection text-to-speech system:
                   `rVoice' from Rhetorical Systems Ltd.},
  CATEGORIES = {join cost, distance measures, MCA, rVoice, edinburgh},
  MONTH = {September},
  PDF = {http://www.cstr.inf.ed.ac.uk/downloads/publications/2002/vepa_icslp02.pdf},
  YEAR = 2002
}

@INPROCEEDINGS{mayoturkwatson:02,
  AUTHOR = {Mayo, C. and Turk, A. and Watson, J.},
  TITLE = {Development of cue weighting strategies in children's
                   speech perception},
  BOOKTITLE = {Proceedings of TIPS: Temporal Integration in the
                   Perception of Speech, Aix-en-Provence},
  CATEGORIES = {speech perception, development, cue weighting},
  YEAR = 2002
}

@ARTICLE{Wright-Hastie_2002,
  AUTHOR = {Helen Wright-Hastie and Massimo Poesio and Stephen
                   Isard},
  TITLE = {Automatically predicting dialogue structure using
                   prosodic features},
  JOURNAL = {Speech Communication},
  VOLUME = 36,
  NUMBER = {1-2},
  PAGES = {63-79},
  CATEGORIES = {dialogue, prosody, recognition},
  YEAR = 2002
}

@PHDTHESIS{Wester-02,
  AUTHOR = {Mirjam Wester},
  TITLE = {Pronunciation Variation Modeling for {D}utch Automatic
                   Speech Recognition},
  SCHOOL = {University of Nijmegen},
  ABSTRACT = {This thesis consists of an introductory review to
                   pronunciation variation modeling, followed by four
                   papers in which the PhD research is described.},
  CATEGORIES = {asr, pm, Nijmegen},
  PDF = {http://www.cstr.inf.ed.ac.uk/downloads/publications/2002/thesis.pdf},
  YEAR = 2002
}

@INPROCEEDINGS{pietquin-icassp02,
  AUTHOR = {O.~Pietquin and S.~Renals},
  TITLE = {{ASR} system modeling for automatic evaluation and
                   optimization of dialogue systems},
  BOOKTITLE = {Proc IEEE ICASSP},
  PAGES = {46--49},
  ABSTRACT = {Though the field of spoken dialogue systems has
                   developed quickly in the last decade, rapid design of
                   dialogue strategies remains uneasy. Several approaches
                   to the problem of automatic strategy learning have been
                   proposed and the use of Reinforcement Learning
                   introduced by Levin and Pieraccini is becoming part of
                   the state of the art in this area. However, the quality
                   of the strategy learned by the system depends on the
                   definition of the optimization criterion and on the
                   accuracy of the environment model. In this paper, we
                   propose to bring a model of an ASR system in the
                   simulated environment in order to enhance the learned
                   strategy. To do so, we introduced recognition error
                   rates and confidence levels produced by ASR systems in
                   the optimization criterion.},
  CATEGORIES = {dialog,rl,sheffield},
  PDF = {http://www.cstr.inf.ed.ac.uk/downloads/publications/2002/icassp02-rl.pdf},
  YEAR = 2002
}

@INPROCEEDINGS{Cox02d,
  AUTHOR = {Cox, S.J. and Lincoln, M. and Tryggvason, J and
                   Nakisa, M and Wells, Mand Tutt, M. and Abbott, S},
  TITLE = {{TESSA}, a system to aid communication with deaf
                   people},
  BOOKTITLE = {ASSETS 2002, Fifth International {ACM SIGCAPH}
                   Conference on Assistive Technologies},
  PAGES = {205-212},
  ADDRESS = {Edinburgh, Scotland},
  ABSTRACT = {{TESSA} is an experimental system that aims to aid
                   transactions between a deaf person and a clerk in a
                   Post Office by translating the clerks speech to sign
                   language. A speech recogniser recognises speech from
                   the clerk and the system then synthesizes the
                   appropriate sequence of signs in British Sign language
                   (BSL) using a speciallydeveloped avatar. By using a
                   phrase lookup approach to language translation, which
                   is appropriate for the highly constrained discourse in
                   a Post Office, we were able to build a working system
                   that we could evaluate. We summarise the results of
                   this evaluation (undertaken by deaf users and Post
                   office clerks), and discuss how the findings from the
                   evaluation are being used in the development of an
                   improved system},
  CATEGORIES = {visicast,sign language,translation,UEA},
  PDF = {http://www.cstr.inf.ed.ac.uk/downloads/publications/2002/Cox-Assets-2000.pdf},
  YEAR = 2002
}

@ARTICLE{robinson-specom02,
  AUTHOR = {A.~J.~Robinson and G.~D.~Cook and D.~P.~W.~Ellis and
                   E.~Fosler-Lussier and S.~J.~Renals and
                   D.~A.~G.~Williams},
  TITLE = {Connectionist Speech Recognition of Broadcast News},
  JOURNAL = {Speech Communication},
  VOLUME = {37},
  PAGES = {27--45},
  ABSTRACT = {This paper describes connectionist techniques for
                   recognition of Broadcast News. The fundamental
                   difference between connectionist systems and more
                   conventional mixture-of-Gaussian systems is that
                   connectionist models directly estimate posterior
                   probabilities as opposed to likelihoods. Access to
                   posterior probabilities has enabled us to develop a
                   number of novel approaches to confidence estimation,
                   pronunciation modelling and search. In addition we have
                   investigated a new feature extraction technique based
                   on the modulation-filtered spectrogram, and methods for
                   combining multiple information sources. We have
                   incorporated all of these techniques into a system for
                   the transcription of Broadcast News, and we present
                   results on the 1998 DARPA Hub-4E Broadcast News
                   evaluation data.},
  CATEGORIES = {sprach,bnews,recognition,am,hybrid,abbot,lm,search,pron,eval,sheffield},
  PDF = {http://www.cstr.inf.ed.ac.uk/downloads/publications/2002/specom02-preprint.pdf},
  PS = {http://www.cstr.inf.ed.ac.uk/downloads/publications/2002/specom02-preprint.ps.gz},
  YEAR = 2002
}

@INPROCEEDINGS{Wester-icslp-02,
  AUTHOR = {M. Wester and J.M. Kessens and H. Strik},
  TITLE = {Goal-directed {ASR} in a multimedia indexing and
                   searching environment ({MUMIS})},
  BOOKTITLE = {Proc. of ICSLP},
  PAGES = {1993-1996},
  ADDRESS = {Denver},
  ABSTRACT = {This paper describes the contribution of automatic
                   speech recognition (ASR) within the framework of MUMIS
                   (Multimedia Indexing and Searching Environment). The
                   domain is football commentaries. The initial results of
                   carrying out ASR on Dutch and English football
                   commentaries are presented. We found that overall word
                   error rates are high, but application specific words
                   are recognized reasonably well. The difficulty of the
                   ASR task is greatly increased by the high levels of
                   noise present in the material.},
  CATEGORIES = {asr, MUMIS, Nijmegen},
  PDF = {http://www.cstr.inf.ed.ac.uk/downloads/publications/2002/wester.2002.2.pdf},
  YEAR = 2002
}

@ARTICLE{Kawamoto2002IPSJ07,
  AUTHOR = {Shin-ichi Kawamoto and Hiroshi Shimodaira and others},
  TITLE = {{Design of Software Toolkit for Anthromorphic Spoken
                   Dialog Agent Software with Customization-oriented
                   Features}},
  JOURNAL = {Information Processing Society of Japan (IPSJ) Journal},
  VOLUME = {43},
  NUMBER = {7},
  PAGES = {2249--2263},
  NOTE = {(in Japanese)},
  MONTH = {Jul},
  YEAR = 2002
}

@INPROCEEDINGS{strom02,
  AUTHOR = {V. Strom},
  TITLE = {From Text to Speech Without {ToBI}},
  BOOKTITLE = {Proc. ICSLP},
  ADDRESS = {Denver},
  ABSTRACT = {A new method for predicting prosodic parameters, i.e.
                   phone durations and F0 targets, from preprocessed text
                   is presented. The prosody model comprises a set of
                   CARTs, which are learned from a large database of
                   labeled speech. This database need not be annotated
                   with Tone and Break Indices (ToBI labels). Instead, a
                   simpler symbolic prosodic description is created by a
                   bootstrapping method. The method had been applied to
                   one Spanish and two German speakers. For the German
                   voices, two listening tests showed a significant
                   preference for the new method over a more traditional
                   approach of prosody prediction, based on hand-crafted
                   rules.},
  PDF = {http://www.cstr.inf.ed.ac.uk/downloads/publications/2002/paper.icslp02.pdf},
  PS = {http://www.cstr.inf.ed.ac.uk/downloads/publications/2002/paper.icslp02.ps},
  YEAR = 2002
}

@ARTICLE{Rokui2002IPSJ07,
  AUTHOR = {Jun Rokui and Mitsuru Nakai and Hiroshi Shimodaira and
                   Shigeki Sagayama},
  TITLE = {{Speaker Normalization Using Linear Transformation of
                   Vocal Tract Length Based on Maximum Likelihood
                   Estimation}},
  JOURNAL = {Information Processing Society of Japan (IPSJ)},
  VOLUME = {43},
  NUMBER = {7},
  PAGES = {2030--2037},
  NOTE = {(in Japanese)},
  ABSTRACT = { },
  CATEGORIES = {asr, jaist},
  MONTH = {Jul},
  YEAR = 2002
}

@PHDTHESIS{richmond2002,
  AUTHOR = {Richmond, K.},
  TITLE = {Estimating Articulatory Parameters from the Acoustic
                   Speech Signal},
  SCHOOL = {The Centre for Speech Technology Research, Edinburgh
                   University},
  CATEGORIES = {artic, ann, mlp, mdn, inversion, mocha, edinburgh},
  ABSTRACT = {A successful method for inferring articulation from the
acoustic speech signal would find many applications: low bit-rate
speech coding, visual representation of speech, and the possibility of
improved automatic speech recognition to name but a few.  It is
unsurprising, therefore, that researchers have been investigating the
acoustic-to-articulatory inversion mapping for several decades now.

A great variety of approaches and models have been applied to the
problem.  Unfortunately, the overwhelming majority of these attempts
have faced difficulties in satisfactorily assessing performance in
terms of genuine human articulation.  However, technologies such as
electromagnetic articulography (EMA) mean that measurement of human
articulation during speech has become increasingly accessible.
Crucially, a large corpus of acoustic-articulatory data during
phonetically-diverse, continuous speech has recently been recorded at
Queen Margaret College, Edinburgh.  One of the primary motivations of
this thesis is to exploit the availability of this remarkable
resource.

Among the data-driven models which have been employed in previous
studies, the feedforward multilayer perceptron (MLP) in particular has
been used several times with promising results.  Researchers have
cited advantages in terms of memory requirement and execution speed as
a significant factor motivating their use.  Furthermore, the MLP is
well known as a universal function approximator; an MLP of suitable
form can in theory represent any arbitrary mapping function.
Therefore, using an MLP in conjunction with the relatively large
quantities of acoustic-articulatory data arguably represents a
promising and useful first research step for the current thesis, and a
significant part of this thesis is occupied with doing this.

Having demonstrated an MLP which performs well enough to provide a
reasonable baseline, we go on to critically evaluate the suitability
of the MLP for the inversion mapping.  The aim is to find ways to
improve modelling accuracy further.  Considering what model of the
target articulatory domain is provided in the MLP is key in this
respect.  It has been shown that the outputs of an MLP trained with
the sum-of-squares error function approximate the mean of the target
data points conditioned on the input vector.  In many situations, this
is an appropriate and sufficient solution.  In other cases, however,
this conditional mean is an inconveniently limiting model of data in
the target domain, particularly for ill-posed problems where the
mapping may be multi-valued.

Substantial evidence exists which shows that multiple articulatory
configurations are able to produce the same acoustic signal.  This
means that a system intended to map from a point in acoustic space can
be faced with multiple candidate articulatory configurations.
Therefore, despite the impressive ability of the MLP to model mapping
functions, it may prove inadequate in certain respects for performing
the acoustic-to-articulatory inversion mapping.  Mixture density
networks (MDN) provide a principled method to model arbitrary
probability density functions over the target domain, conditioned on
the input vector.  In theory, therefore, the MDN offers a superior
model of the target domain compared to the MLP.  We hypothesise that
this advantage will prove beneficial in the case of the
acoustic-to-articulatory inversion mapping.  Accordingly, this thesis
aims to test this hypothesis and directly compare the performance of
MDN with MLP on exactly the same acoustic-to-articulatory inversion
task.},
  PS = {http://www.cstr.inf.ed.ac.uk/downloads/publications/2002/phd_final_bound.ps},
  YEAR = 2002
}

@INPROCEEDINGS{Nakai2002ICPR,
  AUTHOR = {Mitsuru Nakai and Takashi Sudo and Hiroshi Shimodaira
                   and Shigeki Sagayama},
  TITLE = {{Pen Pressure Features for Writer-Independent On-Line
                   Handwriting Recognition Based on Substroke HMM}},
  BOOKTITLE = {Proc. of ICPR2002, III},
  PAGES = {220--223},
  CATEGORIES = {hwr, jaist},
  JOURNAL = {},
  MONTH = {Aug},
  PDF = {http://www.cstr.inf.ed.ac.uk/downloads/publications/2002/Nakai2002ICPR.pdf},
  YEAR = 2002
}

@INPROCEEDINGS{cohen03ngrams,
  ABSTRACT = {In sequential prediction tasks, one repeatedly tries to predict the next element in a sequence. A classical way to solve these problems is to fit an order-n Markov model to the data, but fixed-order models are often bigger than they need to be. In a fixed-order model, all predictors are of length n, even if a shorter predictor would work just as well. We present a greedy algorithm, vpr, for finding variable-length predictive rules. Although vpr is not optimal, we show that on English text, it performs similarly to fixed-order models but uses fewer parameters. },
  AUTHOR = {Paul R. Cohen and Charles Sutton},
  BOOKTITLE = {5th International Symposium on Intelligent Data Analysis},
  PUBLISHER = {Springer-Verlag},
  TITLE = {Very Predictive N-grams for Space-Limited Probabilistic Models},
  URL = {publications/vpr.pdf},
  YEAR = {2003}
}

@INPROCEEDINGS{sutton03aiid,
  ABSTRACT = {Because uncertain reasoning is often intractable, it is hard to reason with a large amount of knowledge. One solution to this problem is to specify a set of possible models, some simple and some complex, and choose which to use based on the problem. We present an architecture for interpreting temporal data, called AIID, that incrementally constructs belief networks based on data that arrives asynchronously. It synthesizes the opportunistic control of the blackboard architecture with recent work on constructing belief networks from fragments. We have implemented this architecture in the domain of military analysis. },
  AUTHOR = {Charles Sutton and Brendan Burns and Clayton Morrison and Paul R. Cohen},
  BOOKTITLE = {5th International Symposium on Intelligent Data Analysis},
  PUBLISHER = {Springer-Verlag},
  TITLE = {Guided Incremental Construction of Belief Networks},
  URL = {publications/aiid-architecture-ida-03.pdf},
  YEAR = {2003}
}

@ARTICLE{sutton01octi,
  ABSTRACT = {Computers are strong players of many strategy games, but in some games, namely Go, success has been more elusive. Thus, it may be fruitful to explore games with intermediate complexity. Octi is such a game. I describe the game of Octi, report results from the 2001 Computer Octi Tournament, and explain why Octi may be a useful domain for research in game AI. },
  AUTHOR = {Charles Sutton},
  JOURNAL = {ICGA Journal},
  KEYWORDS = {games, Octi, tournament reports},
  LOCAL-URL = {file://localhost/Volumes/HERACLITUS/Documents/Charles/research/papers/octi/tourney-2001/tourney-2001.pdf},
  MONTH = {June},
  NUMBER = {2},
  PAGES = {105--112},
  TITLE = {Computers and {Octi}: Report from the 2001 Tournament},
  URL = {publications/tourney-2001.pdf},
  VOLUME = {25},
  YEAR = {2002}
}

@INPROCEEDINGS{cohen:fluents-action,
  ABSTRACT = {Agents need to know the effects of their actions.  Strong associations between actions and effects can be found by counting how often they co-occur.We present an algorithm that learns temporal patterns expressed as \emph{fluents}, propositions with temporal extent. The fluent-learning algorithm is hierarchical and unsupervised.   It works by maintaining co-occurrence statistics on pairs of fluents. In experiments on a mobile robot, the fluent-learning algorithm found temporal associations that correspond to effects of the robot's actions.},
  AUTHOR = {Paul Cohen and Charles Sutton and Brendan Burns},
  BOOKTITLE = {Second International Conference of Development and Learning},
  TITLE = {Learning Effects of Robot Actions Using Temporal Associations},
  URL = {publications/fluents-action.pdf},
  YEAR = {2002}
}

@INPROCEEDINGS{burns03epirob,
  ADDRESS = {Lund, Sweden},
  AUTHOR = {Brendan Burns and Charles Sutton and Clayton Morrison and Paul R. Cohen},
  BOOKTITLE = {Proceedings of the Third International Workshop on Epigenetic Robotics: Modeling Cognitive Development in Robotic Systems},
  EDITOR = {C. G. Prince and L. Berthouze and H. Kozima and D. Bullock and G. Stojanov and C. Balkenius},
  PUBLISHER = {Lund University Cognitive Studies, Volume 101},
  TITLE = {Information Theory and Representation in Associative Word Learning},
  URL = {publications/epigen.pdf},
  YEAR = {2003}
}

@INPROCEEDINGS{mccallum03dcrf,
  ABSTRACT = {Conditional random fields (CRFs) for sequence modeling have several advantages over joint models such as HMMs, including the ability to relax strong independence assumptions made in those models, and the ability to incorporate arbitrary overlapping features. Previous work has focused on linear-chain CRFs, which correspond to finite-state machines, and have efficient exact inference algorithms. Often, however, we wish to label sequence data in multiple interacting ways---for example, performing part-of-speech tagging and noun phrase segmentation simultaneously, increasing joint accuracy by sharing information between them. We present dynamic conditional random fields (DCRFs), which are CRFs in which each time slice has a set of state variables and edges---a distributed state representation as in dynamic Bayesian networks---and parameters are tied across slices. (They could also be called conditionally trained Dynamic Markov Networks.) Since exact inference can be intractable in these models, we perform approximate inference using the tree-based reparameterization framework (TRP). We also present empirical results comparing DCRFs with linear-chain CRFs on natural-language data. },
  AUTHOR = {Andrew McCallum and Khashayar Rohanimanesh and Charles Sutton},
  BOOKTITLE = {NIPS Workshop on Syntax, Semantics, and Statistics},
  MONTH = {December},
  TITLE = {Dynamic Conditional Random Fields for Jointly Labeling Multiple Sequences},
  URL = {publications/dcrf-nips03.pdf},
  YEAR = {2003}
}

@INPROCEEDINGS{sutton04dcrf,
  AUTHOR = { Charles Sutton and Khashayar Rohanimanesh and Andrew McCallum },
  TITLE = {Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data},
  YEAR = 2004,
  URL = {publications/dcrf.pdf},
  BOOKTITLE = {Proceedings of the Twenty-First International Conference on Machine Learning (ICML)},
  ABSTRACT = {In sequence modeling, we often wish to represent complex interaction between
labels, such as when performing multiple, cascaded labeling tasks on the
same sequence, or when long-range dependencies exist.
We present dynamic conditional random fields (DCRFs), a 
generalization of linear-chain conditional random fields (CRFs)
in which each time slice contains a set of state variables and 
edges---a distributed state representation as in dynamic Bayesian 
networks (DBNs)---and parameters are tied across slices.  Since 
exact inference can be intractable in such models, we perform 
approximate inference using several schedules for belief propagation, 
including tree-based reparameterization (TRP).  On a natural-language 
chunking task, we show that a DCRF performs better than a series of 
linear-chain CRFs, achieving comparable performance using only half 
the training data.
  }
}

@TECHREPORT{sutton04skip,
  AUTHOR = { Charles Sutton and Andrew McCallum },
  TITLE = { Collective Segmentation and Labeling of Distant
                  Entities in Information Extraction },
  NOTE = { Presented at ICML Workshop on Statistical Relational Learning
                  and Its Connections to Other Fields },
  YEAR = {2004},
  INSTITUTION = { University of Massachusetts },
  NUMBER = { TR \# 04-49 },
  URL = {publications/tr-04-49.pdf},
  MONTH = { July }
}

@INPROCEEDINGS{welling05cf,
  AUTHOR = {Max Welling and Charles Sutton},
  TITLE = {Learning in {M}arkov Random Fields with Contrastive Free Energies},
  URL = {publications/cf-aistats3.pdf},
  BOOKTITLE = { Tenth International Workshop on Artificial Intelligence and Statistics (AISTATS) },
  YEAR = { 2005 },
  ABSTRACT = { Learning Markov random field (MRF) models is
notoriously hard due to the presence of a global
normalization factor. In this paper we present a
new framework for learning MRF models based
on the contrastive free energy (CF) objective
function. In this scheme the parameters are updated
in an attempt to match the average statistics
of the data distribution and a distribution which
is (partially or approximately) "relaxed" to the
equilibrium distribution. We show that maximum
likelihood, mean field, contrastive divergence
and pseudo-likelihood objectives can be
understood in this paradigm. Moreover, we propose
and study a new learning algorithm: the "kstep
Kikuchi/Bethe approximation". This algorithm
is then tested on a conditional random field
model with "skip-chain" edges to model long
range interactions in text data. It is demonstrated
that with no loss in accuracy, the training time is
brought down from 19 hours (BP based learning)
to 83 minutes, an order of magnitude improvement. }
}

@UNPUBLISHED{sutton04synthesis,
  AUTHOR = {Charles Sutton},
  TITLE = {Conditional probabilistic context-free grammars},
  YEAR = {2004},
  URL = {publications/cscfg.pdf},
  NOTE = {Technical note (synthesis project for admission to Ph.D. candidacy), 2004}
}

@TECHREPORT{mccallum04piecewise,
  AUTHOR = {Andrew McCallum and Charles Sutton},
  TITLE = {Piecewise Training with Parameter Independence Diagrams: Comparing Globally- and Locally-trained Linear-chain {CRFs}},
  INSTITUTION = {Center for Intelligent Information Retrieval, University of Massachusetts},
  NUMBER = {IR-383},
  URL = {publications/lcrf.pdf},
  YEAR = 2004,
  ABSTRACT = { We present a diagrammatic formalism and practial
                  methods for introducing additional independence
                  assumptions into parameter estimation, enabling
                  efficient training of undirected graphical models in
                  locally-normalized pieces. On two real-world data
                  sets we demonstrate our locally-trained linear-chain
                  CRFs outperforming traditional CRFs, training in
                  less than one-fifth the time, and providing a
                  statisticallysignificant gain in accuracy. },
  NOTE = {Presented at NIPS 2004 Workshop on Learning with Structured Outputs}
}

@INPROCEEDINGS{sutton05piecewise,
  AUTHOR = {Charles Sutton and Andrew McCallum},
  TITLE = {Piecewise Training of Undirected Models},
  BOOKTITLE = {21st Conference on Uncertainty in Artificial Intelligence},
  NOTE = { To appear },
  URL = { publications/tip.pdf },
  YEAR = 2005
}

@INPROCEEDINGS{pal05beam,
  AUTHOR = {Chris Pal and Charles Sutton and Andrew McCallum},
  TITLE = {Fast Inference and Learning With Sparse Belief Propagation},
  BOOKTITLE = {Submitted to NIPS-2005},
  URL = { publications/vmd-nips.v5.pdf },
  NOTE = { In submission },
  YEAR = 2005
}

@INPROCEEDINGS{sutton05joint,
  AUTHOR = {Charles Sutton and Andrew McCallum},
  TITLE = {Joint Parsing and Semantic Role Labeling},
  BOOKTITLE = {Proceedings of CoNLL-2005},
  URL = { publications/conll05joint.pdf },
  YEAR = 2005
}

@TECHREPORT{sutton05fast,
  AUTHOR = {Charles Sutton and Andrew McCallum},
  TITLE = {Fast, Piecewise Training for Discriminative Finite-state and Parsing Models},
  INSTITUTION = {Center for Intelligent Information Retrieval},
  YEAR = 2005,
  URL = {publications/nota-ir403.pdf},
  NUMBER = {Technical Report IR-403}
}


%%% ====================================================================
%%% Acknowledgement abbreviations:

@String{ack-nhfb = "Nelson H. F. Beebe,
                    University of Utah,
                    Department of Mathematics, 110 LCB,
                    155 S 1400 E RM 233,
                    Salt Lake City, UT 84112-0090, USA,
                    Tel: +1 801 581 5254,
                    FAX: +1 801 581 4148,
                    e-mail: \path|beebe@math.utah.edu|,
                            \path|beebe@acm.org|,
                            \path|beebe@computer.org| (Internet),
                    URL: \path|http://www.math.utah.edu/~beebe/|"}

%%% ====================================================================
%%% Journal abbreviations:

@String{j-CACM                  = "Communications of the ACM"}

@String{j-TODS                  = "ACM Transactions on Database Systems"}

%%% ====================================================================
%%% Publishers and their addresses:

@String{pub-ACM                 = "ACM Press"}

@String{pub-ACM:adr             = "New York, NY 10036, USA"}

@String{pub-IEEE                = "IEEE Computer Society Press"}

@String{pub-IEEE:adr            = "1109 Spring Street, Suite 300, Silver
                                   Spring, MD 20910, USA"}

@String{pub-MORGAN-KAUFMANN     = "Morgan Kaufmann Publishers"}

@String{pub-MORGAN-KAUFMANN:adr = "Los Altos, CA 94022, USA"}

%%% ====================================================================
%%% Bibliography entries:

@Article{Yao:1977:ABA,
  author =       "S. B. Yao",
  title =        "Approximating Block Accesses in Database
                 Organization",
  journal =      j-CACM,
  volume =       "20",
  number =       "4",
  pages =        "260--261",
  month =        apr,
  year =         "1977",
  CODEN =        "CACMA2",
  ISSN =         "0001-0782",
  bibdate =      "Tue Sep 20 23:14:33 1994",
  bibsource =    "ftp://ftp.ira.uka.de/pub/bibliography/Database/Graefe.bib
                 and
                 ftp://ftp.ira.uka.de/pub/bibliography/Database/Wiederhold.bib",
  note =         "Also published in \cite{Yao:1977:ABM}.",
  keywords =     "selectivity estimation I/O cost query optimization
                 CACM",
}

@Article{Hsiao:1976:ATD,
  author =       "David K. Hsiao",
  title =        "{ACM Transactions on Database Systems}: aim and
                 scope",
  journal =      j-TODS,
  volume =       "1",
  number =       "1",
  pages =        "1--2",
  year =         "1976",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1976-1-1/p1-hsiao/p1-hsiao.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1976-1-1/p1-hsiao/",
  abstract =     "Record-keeping and decision-making in industry and
                 government are increasingly based on data stored in
                 computer processable databases. Thus the need for
                 improved computer technology for building, managing,
                 and using these databases is clearly evident. This need
                 is particularly acute in a complex society where the
                 interrelationships among various aspects of the society
                 must be identified and represented. The data which must
                 be used to represent these relationships are growing
                 more complex in nature and becoming greater in size.
                 Furthermore, the increasing on-line use of computer
                 systems and the proliferation and mass introduction of
                 multilevel secondary storage suggests that future
                 computer systems will be primarily oriented toward
                 database management. The large size of future on-line
                 databases will require the computer system to manage
                 local as well as physical resources. The management of
                 logical resources is concerned with the organization,
                 access, update, storage, and sharing of the data and
                 programs in the database. In addition, the sharing of
                 data means that the database system must be capable of
                 providing privacy protection and of controlling access
                 to the users' data. The term {\em data\/} is
                 interpreted broadly to include textual, numeric, and
                 signal data as well as data found in structured
                 records.\par

                 The aim of {\em ACM Transactions on Database Systems\/}
                 (TODS) is to serve as a focal point for an integrated
                 dissemination of database research and development on
                 storage and processor hardware, system software,
                 applications, information science, information
                 analysis, and file management. These areas are
                 particularly relevant to the following ACM Special
                 Interest Groups: Business Data Processing (SIGBDP),
                 Information Retrieval (SIGIR), and Management of Data
                 (SIGMOD). TODS will also embrace parts of the
                 Management/Database Systems and the Information
                 Retrieval and Language Processing sections of {\em
                 Communications of the ACM}.\par

                 High quality papers on all aspects of computer database
                 systems will be published in TODS. The scope of TODS
                 emphasizes data structures; storage organization; data
                 collection and dissemination; search and retrieval
                 strategies; update strategies; access control
                 techniques; data integrity; security and protection;
                 design and implementation of database software;
                 database related languages including data description
                 languages, query languages, and procedural and
                 nonprocedural data manipulation languages; language
                 processing; analysis and classification of data;
                 database utilities; data translation techniques;
                 distributed database problems and techniques; database
                 recovery and restart; database restructuring; adaptive
                 data structures; concurrent access techniques; database
                 computer hardware architecture; performance and
                 evaluation; intelligent front ends; and related
                 subjects such as privacy and economic issues.",
  acknowledgement = ack-nhfb,
  subject =      "Information Systems --- Database Management ---
                 Systems (H.2.4)",
}

@Article{Chen:1976:ERM,
  author =       "Peter Pin-Shan S. Chen",
  title =        "The Entity-Relationship Model: Toward a Unified View
                 of Data",
  journal =      j-TODS,
  volume =       "1",
  number =       "1",
  pages =        "9--36",
  month =        mar,
  year =         "1976",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Compiler/prog.lang.theory.bib; Database/Graefe.bib;
                 Distributed/gesturing.bib;
                 http://www.acm.org/pubs/toc/; Misc/is.bib;
                 Object/Nierstrasz.bib",
  note =         "Reprinted in \cite{Stonebraker:1988:RDS}.",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1976-1-1/p9-chen/p9-chen.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1976-1-1/p9-chen/",
  abstract =     "A data model, called the entity-relationship model, is
                 proposed. This model incorporates some of the important
                 semantic information about the real world. A special
                 diagrammatic technique is introduced as a tool for
                 database design. An example of database design and
                 description using the model and the diagrammatic
                 technique is given. Some implications for data
                 integrity, information retrieval, and data manipulation
                 are discussed.\par

                 The entity-relationship model can be used as a basis
                 for unification of different views of data: the network
                 model, the relational model, and the entity set model.
                 Semantic ambiguities in these models are analyzed.
                 Possible ways to derive their views of data from the
                 entity-relationship model are presented.",
  acknowledgement = ack-nhfb,
  keywords =     "Data Base Task Group; data definition and
                 manipulation; data integrity and consistency; data
                 models; database design; dblit; entity set model;
                 entity-relationship; entity-relationship model; logical
                 view of data; network model; relational model;
                 semantics of data; TODS",
  subject =      "Information Systems --- Database Management ---
                 Systems (H.2.4): {\bf Relational databases}",
}

@Article{Bayer:1976:EST,
  author =       "R. Bayer and J. K. Metzger",
  title =        "On the Encipherment of Search Trees and Random Access
                 Files",
  journal =      j-TODS,
  volume =       "1",
  number =       "1",
  pages =        "37--52",
  month =        mar,
  year =         "1976",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Database/Wiederhold.bib;
                 http://www.acm.org/pubs/toc/",
  note =         "Also published in \cite[p.~508--510]{Kerr:1975:PIC}.",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1976-1-1/p37-bayer/p37-bayer.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1976-1-1/p37-bayer/",
  abstract =     "The securing of information in indexed, random access
                 files by means of privacy transformations must be
                 considered as a problem distinct from that for
                 sequential files. Not only must processing overhead due
                 to encrypting be considered, but also threats to
                 encipherment arising from updating and the file
                 structure itself must be countered. A general
                 encipherment scheme is proposed for files maintained in
                 a paged structure in secondary storage. This is applied
                 to the encipherment of indexes organized as $B$-trees;
                 a $B$-tree is a particular type of multiway search
                 tree. Threats to the encipherment of $B$-trees,
                 especially relating to updating, are examined, and
                 countermeasures are proposed for each. In addition, the
                 effect of encipherment on file access and update, on
                 paging mechanisms, and on files related to the
                 enciphered index are discussed. Many of the concepts
                 presented may be readily transferred to other forms of
                 multiway index trees and to binary search trees.",
  acknowledgement = ack-nhfb,
  annote =       "Trees versus hashing as his 1974 IFIP paper?",
  keywords =     "",
  subject =      "Software --- Operating Systems --- Security and
                 Protection (D.4.6): {\bf Access controls}; Software ---
                 Operating Systems --- Security and Protection (D.4.6):
                 {\bf Cryptographic controls}",
}

@Article{Lin:1976:DRA,
  author =       "Chyuan Shiun Lin and Diane C. P. Smith and John Miles
                 Smith",
  title =        "The design of a rotating associative memory for
                 relational database applications",
  journal =      j-TODS,
  volume =       "1",
  number =       "1",
  pages =        "53--65",
  year =         "1976",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1976-1-1/p53-lin/p53-lin.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1976-1-1/p53-lin/",
  abstract =     "The design and motivation for a rotating associative
                 relational store (RARES) is described. RARES is
                 designed to enhance the performance of an optimizing
                 relational query interface by supporting important high
                 level optimization techniques. In particular, it can
                 perform tuple selection operations at the storage
                 device and also can provide a mechanism for efficient
                 sorting. Like other designs for rotating associative
                 stores, RARES contains search logic which is attached
                 to the heads of a rotating head-per-track storage
                 device. RARES is distinct from other designs in that it
                 utilizes a novel ``orthogonal'' storage layout. This
                 layout allows a high output rate of selected tuples
                 even when a sort order in the stored relation must be
                 preserved. As in certain other designs, RARES can
                 usually output a tuple as soon as it is found to
                 satisfy the selection criteria. However, relative to
                 these designs, the orthogonal layout allows an order of
                 magnitude reduction in the capacity of storage local to
                 the search logic.",
  acknowledgement = ack-nhfb,
  keywords =     "associative memory; content addressability; data
                 organization; head-per-track disks; memory systems;
                 relational database; rotating devices; search logic;
                 sorting technique",
  subject =      "Information Systems --- Database Management ---
                 Systems (H.2.4): {\bf Relational databases}",
}

@Article{Mahmoud:1976:OAR,
  author =       "Samy Mahmoud and J. S. Riordon",
  title =        "Optimal Allocation of Resources in Distributed
                 Information Networks",
  journal =      j-TODS,
  volume =       "1",
  number =       "1",
  pages =        "66--78",
  month =        mar,
  year =         "1976",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Database/Wiederhold.bib;
                 http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1976-1-1/p66-mahmoud/p66-mahmoud.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1976-1-1/p66-mahmoud/",
  abstract =     "The problems of file allocation and capacity
                 assignment in a fixed topology distributed computer
                 network are examined. These two aspects of the design
                 are tightly coupled by means of an average message
                 delay constraint. The objective is to allocate copies
                 of information files to network nodes and capacities to
                 network links so that a minimum cost is achieved
                 subject to network delay and file availability
                 constraints. A model for solving the problem is
                 formulated and the resulting optimization problem is
                 shown to fall into a class of nonlinear integer
                 programming problems. Deterministic techniques for
                 solving this class of problems are computationally
                 cumbersome, even for small size problems. A new
                 heuristic algorithm is developed, which is based on a
                 decomposition technique that greatly reduces the
                 computational complexity of the problem. Numerical
                 results for a variety of network configurations
                 indicate that the heuristic algorithm, while not
                 theoretically convergent, yields practicable low cost
                 solutions with substantial savings in computer
                 processing time and storage requirements. Moreover, it
                 is shown that this algorithm is capable of solving
                 realistic network problems whose solutions using
                 deterministic techniques are computationally
                 intractable.",
  acknowledgement = ack-nhfb,
  keywords =     "data files; distributed computed; information
                 networks; link capacities; resource sharing",
  subject =      "Information Systems --- Information Storage and
                 Retrieval --- Information Storage (H.3.2)",
}

@Article{Stemple:1976:DMF,
  author =       "David W. Stemple",
  title =        "A Database Management Facility for Automatic
                 Generation of Database Managers",
  journal =      j-TODS,
  volume =       "1",
  number =       "1",
  pages =        "79--94",
  month =        mar,
  year =         "1976",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Database/Wiederhold.bib;
                 http://www.acm.org/pubs/toc/",
  note =         "Also published in \cite[p.~252]{Kerr:1975:PIC}.",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1976-1-1/p79-stemple/p79-stemple.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1976-1-1/p79-stemple/",
  abstract =     "A facility is described for the implementation of
                 database management systems having high degrees of {\em
                 horizontal\/} data independence, i.e. independence from
                 chosen logical properties of a database as opposed to
                 {\em vertical\/} independence from storage structures.
                 The facility consists of a high level language for the
                 specification of virtual database managers, a compiler
                 from this language to a pseudomachine language, and an
                 interpreter for the pseudomachine language.\par

                 It is shown how this facility can be used to produce
                 efficient database management systems with any degree
                 of both horizontal and vertical data independence. Two
                 key features of this tool are the compilation of
                 tailored database managers from individual schemas and
                 multiple levels of optional binding.",
  acknowledgement = ack-nhfb,
  annote =       "Describes SLUSH and SLIM, a proposed compiler and
                 interpreter to operate on network schemas with
                 adjustable binding times.",
  keywords =     "data independence; database management systems",
  subject =      "Information Systems --- Database Management ---
                 Systems (H.2.4); Information Systems --- Database
                 Management (H.2); Software --- Operating Systems ---
                 Systems Programs and Utilities (D.4.9): {\bf make}",
}

@Article{Astrahan:1976:SRR,
  author =       "M. M. Astrahan and M. W. Blasgen and D. D. Chamberlin
                 and K. P. Eswaran and J. N. Gray and P. P. Griffiths
                 and W. F. King and R. A. Lorie and P. R. McJones and J.
                 W. Mehl and G. R. Putzolu and I. L. Traiger and B. W.
                 Wade and V. Watson",
  title =        "{System R}: {A} Relational Approach to Database
                 Management",
  journal =      j-TODS,
  volume =       "1",
  number =       "2",
  pages =        "97--137",
  month =        jun,
  year =         "1976",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Database/Graefe.bib; Database/Wiederhold.bib;
                 http://www.acm.org/pubs/toc/; Object/Nierstrasz.bib",
  note =         "Also published in/as: IBM, San Jose, Research Report.
                 No. RJ-1738, Feb. 1976. Reprinted in
                 \cite{Stonebraker:1988:RDS}.",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1976-1-2/p97-astrahan/p97-astrahan.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1976-1-2/p97-astrahan/",
  abstract =     "System R is a database management system which
                 provides a high level relational data interface. The
                 systems provides a high level of data independence by
                 isolating the end user as much as possible from
                 underlying storage structures. The system permits
                 definition of a variety of relational views on common
                 underlying data. Data control features are provided,
                 including authorization, integrity assertions,
                 triggered transactions, a logging and recovery
                 subsystem, and facilities for maintaining data
                 consistency in a shared-update environment.\par

                 This paper contains a description of the overall
                 architecture and design of the system. At the present
                 time the system is being implemented and the design
                 evaluated. We emphasize that System R is a vehicle for
                 research in database architecture, and is not planned
                 as a product.",
  acknowledgement = ack-nhfb,
  keywords =     "authorization; data structures; database; dblit; index
                 structures; locking; nonprocedural language; recovery;
                 relational model; TODS relation database IBM San Jose",
  subject =      "Information Systems --- Database Management ---
                 Systems (H.2.4): {\bf System R}; Information Systems
                 --- Database Management --- Systems (H.2.4): {\bf
                 Relational databases}; Information Systems --- Database
                 Management (H.2)",
}

@Article{Navathe:1976:RLD,
  author =       "Shamkant B. Navathe and James P. Fry",
  title =        "Restructuring for Large Data Bases: Three Levels of
                 Abstraction",
  journal =      j-TODS,
  volume =       "1",
  number =       "2",
  pages =        "138--158",
  month =        mar,
  year =         "1976",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Database/Wiederhold.bib;
                 http://www.acm.org/pubs/toc/",
  note =         "Also published in \cite[p.~174]{Kerr:1975:PIC}.",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1976-1-2/p138-navathe/p138-navathe.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1976-1-2/p138-navathe/",
  abstract =     "The development of a powerful restructuring function
                 involves two important components--the unambiguous
                 specification of the restructuring operations and the
                 realization of these operations in a software system.
                 This paper is directed to the first component in the
                 belief that a precise specification will provide a firm
                 foundation for the development of restructuring
                 algorithms and, subsequently, their implementation. The
                 paper completely defines the semantics of the
                 restructuring of tree structured databases.\par

                 The delineation of the restructuring function is
                 accomplished by formulating three different levels of
                 abstraction, with each level of abstraction
                 representing successively more detailed semantics of
                 the function.\par

                 At the first level of abstraction, the schema
                 modification, three types are identified--naming,
                 combining, and relating; these three types are further
                 divided into eight schema operations. The second level
                 of abstraction, the instance operations, constitutes
                 the transformations on the data instances; they are
                 divided into group operations such as replication,
                 factoring, union, etc., and group relation operations
                 such as collapsing, refinement, fusion, etc. The final
                 level, the item value operations, includes the actual
                 item operations, such as copy value, delete value, or
                 create a null value.",
  acknowledgement = ack-nhfb,
  keywords =     "data definition; data translation; database; database
                 management systems; logical restructuring",
  subject =      "Information Systems --- Database Management (H.2);
                 Information Systems --- Database Management ---
                 Heterogeneous Databases (H.2.5): {\bf Data
                 translation**}",
}

@Article{Yao:1976:DDR,
  author =       "S. B. Yao and K. S. Das and T. J. Teorey",
  title =        "A Dynamic Database Reorganization Algorithm",
  journal =      j-TODS,
  volume =       "1",
  number =       "2",
  pages =        "159--174",
  month =        jun,
  year =         "1976",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Database/Wiederhold.bib;
                 http://www.acm.org/pubs/toc/",
  note =         "Also published in/as: Purdue Un., TR-168, Nov. 1975.",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1976-1-2/p159-yao/p159-yao.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1976-1-2/p159-yao/",
  abstract =     "Reorganization is necessary in some databases for
                 overcoming the performance deterioration caused by
                 updates. The paper presents a dynamic reorganization
                 algorithm which makes the reorganization decision by
                 measuring the database search costs. Previously, the
                 reorganization intervals could only be determined for
                 linear deterioration and known database lifetime. It is
                 shown that the dynamic reorganization algorithm is near
                 optimum for constant reorganization cost and is
                 superior for increasing reorganization cost. In
                 addition, it can be applied to cases of unknown
                 database lifetime and nonlinear performance
                 deterioration. The simplicity, generality, and
                 efficiency appear to make this good heuristic for
                 database reorganization.",
  acknowledgement = ack-nhfb,
  keywords =     "database; file organization; information retrieval;
                 reorganization",
  subject =      "Information Systems --- Information Storage and
                 Retrieval --- Information Storage (H.3.2): {\bf File
                 organization}; Information Systems --- Information
                 Storage and Retrieval --- Information Search and
                 Retrieval (H.3.3): {\bf Retrieval models}",
}

@Article{Burkhard:1976:HTA,
  author =       "Walter A. Burkhard",
  title =        "Hashing and Trie Algorithms for Partial-Match
                 Retrieval",
  journal =      j-TODS,
  volume =       "1",
  number =       "2",
  pages =        "175--187",
  month =        jun,
  year =         "1976",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Database/Wiederhold.bib; Graphics/siggraph/76.bib;
                 http://www.acm.org/pubs/toc/",
  note =         "Also published in/as: UCSD, Appl. Physics and Inf. Sc,
                 CS TR.2, Jun. 1975.",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1976-1-2/p175-burkhard/p175-burkhard.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1976-1-2/p175-burkhard/",
  abstract =     "File designs suitable for retrieval from a file of
                 $k$-letter words when queries may be only partially
                 specified are examined. A new class of partial match
                 file designs (called PMF designs) based upon hash
                 coding and trie search algorithms which provide good
                 worst-case performance is introduced. Upper bounds on
                 the worst-case performance of these designs are given
                 along with examples of files achieving the bound. Other
                 instances of PMF designs are known to have better
                 worst-case performances. The implementation of the file
                 designs with associated retrieval algorithms is
                 considered. The amount of storage required is
                 essentially that required of the records themselves.",
  acknowledgement = ack-nhfb,
  keywords =     "algorithms; analysis; associative retrieval; hash
                 coding; partial match; retrieval; searching; trie
                 search",
  oldlabel =     "geom-96",
  subject =      "Mathematics of Computing --- Mathematical Software
                 (G.4): {\bf Algorithm design and analysis}; Information
                 Systems --- Information Storage and Retrieval ---
                 Information Search and Retrieval (H.3.3): {\bf
                 Retrieval models}",
}

@Article{Stonebraker:1976:DII,
  author =       "Michael Stonebraker and Eugene Wong and Peter Kreps
                 and Gerald Held",
  title =        "The Design and Implementation of {INGRES}",
  journal =      j-TODS,
  volume =       "1",
  number =       "3",
  pages =        "189--222",
  month =        sep,
  year =         "1976",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Database/Graefe.bib; Database/Wiederhold.bib;
                 http://www.acm.org/pubs/toc/; Parallel/Multi.bib",
  note =         "Reprinted in \cite{Stonebraker:1988:RDS}. Also
                 published in/as: UCB, Elec. Res. Lab, Memo No.
                 ERL-M577, Jan. 1976.",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1976-1-3/p189-stonebraker/p189-stonebraker.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1976-1-3/p189-stonebraker/",
  abstract =     "The currently operational (March 1976) version of the
                 INGRES database management system is described. This
                 multiuser system gives a relational view of data,
                 supports two high level nonprocedural data
                 sublanguages, and runs as a collection of user
                 processes on top of the UNIX operating system for
                 Digital Equipment Corporation PDP 11/40, 11/45, and
                 11/70 computers. Emphasis is on the design decisions
                 and tradeoffs related to (1) structuring the system
                 into processes, (2) embedding one command language in a
                 general purpose programming language, (3) the
                 algorithms implemented to process interactions, (4) the
                 access methods implemented, (5) the concurrency and
                 recovery control currently provided, and (6) the data
                 structures used for system catalogs and the role of the
                 database administrator.\par

                 Also discussed are (1) support for integrity
                 constraints (which is only partly operational), (2) the
                 not yet supported features concerning views and
                 protection, and (3) future plans concerning the
                 system.",
  acknowledgement = ack-nhfb,
  annote =       "Describes implementation of INGRES, a non-distributed
                 relational database system. This paper is useful for
                 understanding the distributed INGRES paper.",
  keywords =     "concurrency; data integrity; data organization; data
                 sublanguage; database optimization; nonprocedural
                 language; protection; QUEL EQUEL query modification
                 process structure Halloween problem TODS; query
                 decomposition; query language; relational database",
  subject =      "Information Systems --- Database Management ---
                 Systems (H.2.4): {\bf Relational databases};
                 Information Systems --- Database Management ---
                 Languages (H.2.3); Information Systems --- Database
                 Management --- General (H.2.0): {\bf Security,
                 integrity, and protection**}",
}

@Article{Wong:1976:DSQ,
  author =       "Eugene Wong and Karel Youssefi",
  title =        "Decomposition --- {A} Strategy for Query Processing",
  journal =      j-TODS,
  volume =       "1",
  number =       "3",
  pages =        "223--241",
  month =        sep,
  year =         "1976",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Database/Graefe.bib; Database/Wiederhold.bib;
                 http://www.acm.org/pubs/toc/",
  note =         "Also published in/as: UCB, Elec. Res. Lab, Memo No.
                 ERL-574, Jan. 1976;",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1976-1-3/p223-wong/p223-wong.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1976-1-3/p223-wong/",
  abstract =     "Strategy for processing multivariable queries in the
                 database management system INGRES is considered. The
                 general procedure is to decompose the query into a
                 sequence of one-variable queries by alternating between
                 (a) reduction: breaking off components of the query
                 which are joined to it by a single variable, and (b)
                 tuple substitution: substituting for one of the
                 variables a tuple at a time. Algorithms for reduction
                 and for choosing the variable to be substituted are
                 given. In most cases the latter decision depends on
                 estimation of costs; heuristic procedures for making
                 such estimates are outlined.",
  acknowledgement = ack-nhfb,
  annote =       "INGRES query decomposition by reduction to single
                 variable queries, and tuple substitution --- choosing a
                 variable and for it from all tuples, generating a
                 family of queries in one fewer variable.",
  keywords =     "connected query; decomposition; detachment; Ingres
                 TODS; irreducible query; joining (overlapping)
                 variable; query processing; relational database; tuple
                 substitution; variable selection",
  subject =      "Information Systems --- Database Management ---
                 Systems (H.2.4): {\bf Relational databases};
                 Information Systems --- Database Management --- Systems
                 (H.2.4): {\bf Query processing}",
}

@Article{Griffiths:1976:AMR,
  author =       "Patricia P. Griffiths and Bradford W. Wade",
  title =        "An Authorization Mechanism for a Relational Database
                 System",
  journal =      j-TODS,
  volume =       "1",
  number =       "3",
  pages =        "242--255",
  month =        sep,
  year =         "1976",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Database/Wiederhold.bib;
                 http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1976-1-3/p242-griffiths/p242-griffiths.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1976-1-3/p242-griffiths/",
  abstract =     "A multiuser database system must selectively permit
                 users to share data, while retaining the ability to
                 restrict data access. There must be a mechanism to
                 provide protection and security, permitting information
                 to be accessed only by properly authorized users.
                 Further, when tables or restricted views of tables are
                 created and destroyed dynamically, the granting,
                 authentication, and revocation of authorization to use
                 them must also be dynamic. Each of these issues and
                 their solutions in the context of the relational
                 database management system System R are discussed.
                 \par

                 When a database user creates a table, he is fully and
                 solely authorized to perform upon it actions such as
                 read, insert, update, and delete. He may explicitly
                 grant to any other user any or all of his privileges on
                 the table. In addition he may specify that that user is
                 authorized to further grant these privileges to still
                 other users. The result is a directed graph of granted
                 privileges originating from the table creator.\par

                 At some later time a user A may revoke some or all of
                 the privileges which he previously granted to another
                 user B. This action usually revokes the entire subgraph
                 of the grants originating from A's grant to B. It may
                 be, however, that B will still possess the revoked
                 privileges by means of a grant from another user C, and
                 therefore some or all of B's grants should not be
                 revoked. This problem is discussed in detail, and an
                 algorithm for detecting exactly which of B's grants
                 should be revoked is presented.",
  acknowledgement = ack-nhfb,
  annote =       "Defines a dynamic authorization mechanism. A database
                 user can grant or revoke privileges (such as to read,
                 insert, or delete) on a file that he has created.
                 Furthermore, he can authorize others to grant these
                 same privileges. The database management system keeps
                 track of a directed graph, emanating from the creator
                 of granted privileges.",
  keywords =     "access control; authorization; data dependent
                 authorization; database systems; privacy; protection in
                 databases; revocation of authorization; security",
  subject =      "Information Systems --- Database Management ---
                 Systems (H.2.4): {\bf Relational databases};
                 Information Systems --- Database Management --- Systems
                 (H.2.4); Information Systems --- Database Management
                 --- General (H.2.0): {\bf Security, integrity, and
                 protection**}",
}

@Article{Severance:1976:DFT,
  author =       "Dennis G. Severance and Guy M. Lohman",
  title =        "Differential Files: Their Application to the
                 Maintenance of Large Databases",
  journal =      j-TODS,
  volume =       "1",
  number =       "3",
  pages =        "256--267",
  month =        sep,
  year =         "1976",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Database/Graefe.bib; Database/Wiederhold.bib;
                 http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1976-1-3/p256-severance/p256-severance.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1976-1-3/p256-severance/",
  abstract =     "The representation of a collection of data in terms of
                 its differences from some preestablished point of
                 reference is a basic storage compaction technique which
                 finds wide applicability. This paper describes a
                 differential database representation which is shown to
                 be an efficient method for storing large and volatile
                 databases. The technique confines database
                 modifications to a relatively small area of physical
                 storage and as a result offers two significant
                 operational advantages. First, because the ``reference
                 point'' for the database is inherently static, it can
                 be simply and efficiently stored. Second, since all
                 modifications to the database are physically localized,
                 the process of backup and the process of recovery are
                 relatively fast and inexpensive.",
  acknowledgement = ack-nhfb,
  keywords =     "backup and recovery; data sharing; database
                 maintenance; differential files",
  subject =      "Information Systems --- Database Management (H.2);
                 Information Systems --- Information Storage and
                 Retrieval --- Information Storage (H.3.2): {\bf File
                 organization}",
}

@Article{Shneiderman:1976:BSS,
  author =       "Ben Shneiderman and Victor Goodman",
  title =        "Batched Searching of Sequential and Tree Structured
                 Files",
  journal =      j-TODS,
  volume =       "1",
  number =       "3",
  pages =        "268--275",
  month =        sep,
  year =         "1976",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Database/Wiederhold.bib;
                 http://www.acm.org/pubs/toc/",
  note =         "See comments in \cite{Piwowarski:1985:CBS}. Also
                 published in/as: Indiana Un., CSD Tech. Ref. 0132.",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1976-1-3/p268-shneiderman/p268-shneiderman.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1976-1-3/p268-shneiderman/",
  abstract =     "The technique of batching searches has been ignored in
                 the context of disk based online data retrieval
                 systems. This paper suggests that batching be
                 reconsidered for such systems since the potential
                 reduction in processor demand may actually reduce
                 response time. An analysis with sample numerical
                 results and algorithms is presented.",
  acknowledgement = ack-nhfb,
  keywords =     "",
  subject =      "Information Systems --- Information Storage and
                 Retrieval --- Information Storage (H.3.2): {\bf File
                 organization}; Information Systems --- Information
                 Storage and Retrieval (H.3)",
}

@Article{Bernstein:1976:STN,
  author =       "Philip A. Bernstein",
  title =        "Synthesizing Third Normal Form Relations from
                 Functional Dependencies",
  journal =      j-TODS,
  volume =       "1",
  number =       "4",
  pages =        "277--298",
  month =        dec,
  year =         "1976",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Database/Wiederhold.bib; Distributed/gesturing.bib;
                 http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1976-1-4/p277-bernstein/p277-bernstein.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1976-1-4/p277-bernstein/",
  abstract =     "It has been proposed that the description of a
                 relational database can be formulated as a set of
                 functional relationships among database attributes.
                 These functional relationships can then be used to
                 synthesize algorithmically a relational scheme. It is
                 the purpose of this paper to present an effective
                 procedure for performing such a synthesis. The schema
                 that results from this procedure is proved to be in
                 Codd's third normal form and to contain the fewest
                 possible number of relations. Problems with earlier
                 attempts to construct such a procedure are also
                 discussed.",
  acknowledgement = ack-nhfb,
  keywords =     "database schema; functional dependency; relational
                 model; semantics of data; third normal form",
  subject =      "Information Systems --- Database Management ---
                 Logical Design (H.2.1): {\bf Normal forms}; Information
                 Systems --- Database Management --- Logical Design
                 (H.2.1): {\bf Schema and subschema}; Information
                 Systems --- Database Management --- Logical Design
                 (H.2.1): {\bf Data models}",
}

@Article{Liu:1976:APS,
  author =       "Jane W. S. Liu",
  title =        "Algorithms for parsing search queries in systems with
                 inverted file organization",
  journal =      j-TODS,
  volume =       "1",
  number =       "4",
  pages =        "299--316",
  year =         "1976",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1976-1-4/p299-liu/p299-liu.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1976-1-4/p299-liu/",
  abstract =     "In an inverted file system a query is in the form of a
                 Boolean expression of index terms. In response to a
                 query the system accesses the inverted lists
                 corresponding to the index terms, merges them, and
                 selects from the merged list those records that satisfy
                 the search logic. Considered in this paper is the
                 problem of determining a Boolean expression which leads
                 to the minimum total merge time among all Boolean
                 expressions that are equivalent to the expression given
                 in the query. This problem is the same as finding an
                 optimal merge tree among all trees that realize the
                 truth function determined by the Boolean expression in
                 the query. Several algorithms are described which
                 generate optimal merge trees when the sizes of overlaps
                 between different lists are small compared with the
                 length of the lists.",
  acknowledgement = ack-nhfb,
  keywords =     "inverted file systems; merge algorithms; parsing
                 Boolean queries",
  subject =      "Information Systems --- Information Storage and
                 Retrieval --- Information Storage (H.3.2): {\bf File
                 organization}; Mathematics of Computing ---
                 Mathematical Software (G.4): {\bf Algorithm design and
                 analysis}; Information Systems --- Database Management
                 --- Systems (H.2.4): {\bf Query processing}",
}

@Article{Sherman:1976:PDM,
  author =       "Stephen W. Sherman and Richard S. Brice",
  title =        "Performance of a Database Manager in a Virtual Memory
                 System",
  journal =      j-TODS,
  volume =       "1",
  number =       "4",
  pages =        "317--343",
  month =        dec,
  year =         "1976",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Database/Graefe.bib; http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1976-1-4/p317-sherman/p317-sherman.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1976-1-4/p317-sherman/",
  abstract =     "Buffer space is created and managed in database
                 systems in order to reduce accesses to the I/O devices
                 for database information. In systems using virtual
                 memory any increase in the buffer space may be
                 accompanied by an increase in paging. The effects of
                 these factors on system performance are quantified
                 where system performance is a function of page faults
                 and database accesses to I/O devices. This phenomenon
                 is examined through the analysis of empirical data
                 gathered in a multifactor experiment. The factors
                 considered are memory size, size of buffer space,
                 memory replacement algorithm, and buffer management
                 algorithm. The improvement of system performance
                 through an increase in the size of the buffer space is
                 demonstrated. It is also shown that for certain values
                 of the other factors an increase in the size of the
                 buffer space can cause performance to deteriorate.",
  acknowledgement = ack-nhfb,
  keywords =     "buffer manager; Buffer operating system support TODS;
                 database management; double paging; page faults; page
                 replacement algorithm; performance; virtual buffer;
                 virtual memory",
  subject =      "Information Systems --- Database Management ---
                 Systems (H.2.4): {\bf Database Manager}; Mathematics of
                 Computing --- Mathematical Software (G.4): {\bf
                 Algorithm design and analysis}; Computer Systems
                 Organization --- Performance of Systems (C.4)",
}

@Article{Donovan:1976:DSA,
  author =       "John J. Donovan",
  title =        "Database System Approach to Management Decision
                 Support",
  journal =      j-TODS,
  volume =       "1",
  number =       "4",
  pages =        "344--369",
  month =        dec,
  year =         "1976",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Database/Wiederhold.bib;
                 http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1976-1-4/p344-donovan/p344-donovan.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1976-1-4/p344-donovan/",
  abstract =     "Traditional intuitive methods of decision-making are
                 no longer adequate to deal with the complex problems
                 faced by the modern policymaker. Thus systems must be
                 developed to provide the information and analysis
                 necessary for the decisions which must be made. These
                 systems are called decision support systems. Although
                 database systems provide a key ingredient to decision
                 support systems, the problems now facing the
                 policymaker are different from those problems to which
                 database systems have been applied in the past. The
                 problems are usually not known in advance, they are
                 constantly changing, and answers are needed quickly.
                 Hence additional technologies, methodologies, and
                 approaches must expand the traditional areas of
                 database and operating systems research (as well as
                 other software and hardware research) in order for them
                 to become truly effective in supporting policymakers.
                 \par

                 This paper describes recent work in this area and
                 indicates where future work is needed. Specifically the
                 paper discusses: (1) why there exists a vital need for
                 decision support systems; (2) examples from work in the
                 field of energy which make explicit the characteristics
                 which distinguish these decision support systems from
                 traditional operational and managerial systems; (3) how
                 an awareness of decision support systems has evolved,
                 including a brief review of work done by others and a
                 statement of the computational needs of decision
                 support systems which are consistent with contemporary
                 technology; (4) an approach which has been made to meet
                 many of these computational needs through the
                 development and implementation of a computational
                 facility, the Generalized Management Information System
                 (GMIS); and (5) the application of this computational
                 facility to a complex and important energy problem
                 facing New England in a typical study within the New
                 England Energy Management Information System (NEEMIS)
                 Project.",
  acknowledgement = ack-nhfb,
  keywords =     "database systems; decision support systems; management
                 applications; modeling; networking; relational; virtual
                 machines",
  subject =      "Information Systems --- Database Management (H.2);
                 Information Systems --- Database Management --- Systems
                 (H.2.4)",
}

@Article{McGee:1976:UCD,
  author =       "William C. McGee",
  title =        "On user criteria for data model evaluation",
  journal =      j-TODS,
  volume =       "1",
  number =       "4",
  pages =        "370--387",
  year =         "1976",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1976-1-4/p370-mcgee/p370-mcgee.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1976-1-4/p370-mcgee/",
  abstract =     "The emergence of a database technology in recent years
                 has focused interest on the subject of data models. A
                 data model is the class of logical data structures
                 which a computer system or language makes available to
                 the user for the purpose of formulating data processing
                 applications. The diversity of computer systems and
                 languages has resulted in a corresponding diversity of
                 data models, and has created a problem for the user in
                 selecting a data model which is in some sense
                 appropriate to a given application. An evaluation
                 procedure is needed which will allow the user to
                 evaluate alternative models in the context of a
                 specific set of applications. This paper takes a first
                 step toward such a procedure by identifying the
                 attributes of a data model which can be used as
                 criteria for evaluating the model. Two kinds of
                 criteria are presented: use criteria, which measure the
                 usability of the model; and implementation criteria,
                 which measure the implementability of the model and the
                 efficiency of the resulting implementation. The use of
                 the criteria is illustrated by applying them to three
                 specific models: an $n$-ary relational model, a
                 hierarchic model, and a network model.",
  acknowledgement = ack-nhfb,
  keywords =     "data model; data model evaluation; data model
                 selection; hierarchic model; network model; relational
                 model",
  subject =      "Information Systems --- Database Management ---
                 Logical Design (H.2.1): {\bf Data models}",
}

@Article{Kam:1977:MSD,
  author =       "John B. Kam and Jeffrey D. Ullman",
  title =        "A Model of Statistical Databases and Their Security",
  journal =      j-TODS,
  volume =       "2",
  number =       "1",
  pages =        "1--10",
  month =        mar,
  year =         "1977",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Database/Graefe.bib; http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1977-2-1/p1-kam/p1-kam.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1977-2-1/p1-kam/",
  abstract =     "Considered here, for a particular model of databases
                 in which only information about relatively large sets
                 of records can be obtained, is the question of whether
                 one can from statistical information obtain information
                 about individuals. Under the assumption that the data
                 in the database is taken from arbitrary integers, it is
                 shown that essentially nothing can be inferred. It is
                 also shown that when the values are known to be
                 imprecise in some fixed range, one can often deduce the
                 values of individual records.",
  acknowledgement = ack-nhfb,
  keywords =     "compromisability; data security; linear independence;
                 statistical database; vector spece",
  subject =      "Information Systems --- Database Management ---
                 Database Applications (H.2.8): {\bf Statistical
                 databases}; Information Systems --- Database Management
                 --- General (H.2.0): {\bf Security, integrity, and
                 protection**}",
}

@Article{Bayer:1977:PBT,
  author =       "Rudolf Bayer and Karl Unterauer",
  title =        "Prefix {B}-trees",
  journal =      j-TODS,
  volume =       "2",
  number =       "1",
  pages =        "11--26",
  month =        mar,
  year =         "1977",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Database/Graefe.bib; Database/Wiederhold.bib;
                 http://www.acm.org/pubs/toc/",
  note =         "Also published in/as: IBM Yorktwon, Technical Report
                 RJ1796, Jun. 1976.",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1977-2-1/p11-bayer/p11-bayer.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1977-2-1/p11-bayer/",
  abstract =     "Two modifications of $B$-trees are described, simple
                 prefix $B$-trees and prefix $B$-trees. Both store only
                 parts of keys, namely prefixes, in the index part of a
                 $B$*-tree. In simple prefix $B$-trees those prefixes
                 are selected carefully to minimize their length. In
                 prefix $B$-trees the prefixes need not be fully stored,
                 but are reconstructed as the tree is searched. Prefix
                 $B$-trees are designed to combine some of the
                 advantages of $B$-trees, digital search trees, and key
                 compression techniques while reducing the processing
                 overhead of compression techniques.",
  acknowledgement = ack-nhfb,
  annote =       "Index Btree structures can easily be compressed.",
  keywords =     "truncation compression TODS",
  subject =      "Data --- Data Structures (E.1): {\bf Trees}",
}

@Article{Schkolnick:1977:CAH,
  author =       "Mario Schkolnick",
  title =        "A Clustering Algorithm for Hierarchical Structures",
  journal =      j-TODS,
  volume =       "2",
  number =       "1",
  pages =        "27--44",
  month =        may,
  year =         "1977",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Tue Dec 10 09:36:45 1996",
  bibsource =    "Database/Graefe.bib; Database/Wiederhold.bib",
  annote =       "Optimal file partitioning, applied to IMS.",
}

@Article{Yao:1977:ABM,
  author =       "S. B. Yao",
  title =        "An Attribute Based Model for Database Access Cost
                 Analysis",
  journal =      j-TODS,
  volume =       "2",
  number =       "1",
  pages =        "45--67",
  month =        mar,
  year =         "1977",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Database/Graefe.bib; Database/Wiederhold.bib;
                 http://www.acm.org/pubs/toc/",
  note =         "Also published in \cite{Yao:1977:ABA}.",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1977-2-1/p45-yao/p45-yao.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1977-2-1/p45-yao/",
  abstract =     "A generalized model for physical database
                 organizations is presented. Existing database
                 organizations are shown to fit easily into the model as
                 special cases. Generalized access algorithms and cost
                 equations associated with the model are developed and
                 analyzed. The model provides a general design framework
                 in which the distinguishing properties of database
                 organizations are made explicit and their performances
                 can be compared.",
  acknowledgement = ack-nhfb,
  keywords =     "B-tree; database model; database organization;
                 database performance; estimation approximation TODS;
                 evaluation; index organization; index sequential;
                 inverted file; multilist",
  subject =      "Information Systems --- Database Management ---
                 Logical Design (H.2.1); Information Systems ---
                 Information Storage and Retrieval --- Content Analysis
                 and Indexing (H.3.1); Data --- Data Structures (E.1):
                 {\bf Trees}",
}

@Article{Anderson:1977:MCS,
  author =       "Henry D. Anderson and P. Bruce Berra",
  title =        "Minimum Cost Selection of Secondary Indexes for
                 Formatted Files",
  journal =      j-TODS,
  volume =       "2",
  number =       "1",
  pages =        "68--90",
  month =        mar,
  year =         "1977",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Database/Graefe.bib; Database/Wiederhold.bib;
                 http://www.acm.org/pubs/toc/; Misc/is.bib",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1977-2-1/p68-anderson/p68-anderson.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1977-2-1/p68-anderson/",
  abstract =     "Secondary indexes are often used in database
                 management systems for secondary key retrieval.
                 Although their use can improve retrieval time
                 significantly, the cost of index maintenance and
                 storage increases the overhead of the file processing
                 application. The optimal set of indexed secondary keys
                 for a particular application depends on a number of
                 application dependent factors. In this paper a cost
                 function is developed for the evaluation of candidate
                 indexing choices and applied to the optimization of
                 index selection. Factors accounted for include file
                 size, the relative rates of retrieval and maintenance
                 and the distribution of retrieval and maintenance over
                 the candidate keys, index structure, and system
                 charging rates. Among the results demonstrated are the
                 increased effectiveness of secondary indexes for large
                 files, the effect of the relative rates of retrieval
                 and maintenance, the greater cost of allowing for
                 arbitrarily formulated queries, and the impact on cost
                 of the use of different index structures.",
  acknowledgement = ack-nhfb,
  keywords =     "access methods; access path; Boolean query; cost
                 function; data management; database; file design; file
                 organization; inverted file; inverted index;
                 maintenance; optimization; retrieval; secondary index;
                 secondary key; secondary key access",
  subject =      "Information Systems --- Information Storage and
                 Retrieval --- Content Analysis and Indexing (H.3.1):
                 {\bf Indexing methods}; Information Systems ---
                 Information Storage and Retrieval --- Information
                 Storage (H.3.2): {\bf File organization}; Information
                 Systems --- Database Management --- Physical Design
                 (H.2.2): {\bf Access methods}; Information Systems ---
                 Database Management (H.2)",
}

@Article{Lorie:1977:PIL,
  author =       "Raymond A. Lorie",
  title =        "Physical Integrity in a Large Segmented Database",
  journal =      j-TODS,
  volume =       "2",
  number =       "1",
  pages =        "91--104",
  month =        mar,
  year =         "1977",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Database/Graefe.bib; http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1977-2-1/p91-lorie/p91-lorie.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1977-2-1/p91-lorie/",
  abstract =     "A database system can generally be divided into three
                 major components. One component supports the logical
                 database as seen by the user. Another component maps
                 the information into physical records. The third
                 component, called the storage component, is responsible
                 for mapping these records onto auxiliary storage
                 (generally disks) and controlling their transfer to and
                 from main storage.\par

                 This paper is primarily concerned with the
                 implementation of a storage component. It considers a
                 simple and classical interface to the storage
                 component: Seen at this level the database is a
                 collection of segments. Each segment is a linear
                 address space.\par

                 A recovery scheme is first proposed for system failure
                 (hardware or software error which causes the contents
                 of main storage to be lost). It is based on maintaining
                 a dual mapping between pages and their location on
                 disk. One mapping represents the current state of a
                 segment being modified; the other represents a previous
                 backup state. At any time the backup state can be
                 replaced by the current state without any data merging.
                 Procedures for segment modification, save, and restore
                 are analyzed. Another section proposes a facility for
                 protection against damage to the auxiliary storage
                 itself. It is shown how such protection can be obtained
                 by copying on a tape (checkpoint) only those pages that
                 have been modified since the last checkpoint.",
  acknowledgement = ack-nhfb,
  keywords =     "checkpoint-restart; database; recovery; storage
                 management",
  subject =      "Information Systems --- Database Management ---
                 General (H.2.0): {\bf Security, integrity, and
                 protection**}; Information Systems --- Information
                 Storage and Retrieval --- Information Storage (H.3.2);
                 Information Systems --- Database Management ---
                 Physical Design (H.2.2): {\bf Recovery and restart}",
}

@Article{Smith:1977:DAA,
  author =       "John Miles Smith and Diane C. P. Smith",
  title =        "Database abstractions: Aggregation and
                 Generalization",
  journal =      j-TODS,
  volume =       "2",
  number =       "2",
  pages =        "105--133",
  month =        jun,
  year =         "1977",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Database/Graefe.bib; Database/Wiederhold.bib;
                 Distributed/gesturing.bib;
                 http://www.acm.org/pubs/toc/; Object/Nierstrasz.bib",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1977-2-2/p105-smith/p105-smith.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1977-2-2/p105-smith/",
  abstract =     "Two kinds of abstraction that are fundamentally
                 important in database design and usage are defined.
                 Aggregation is an abstraction which turns a
                 relationship between objects into an aggregate object.
                 Generalization is an abstraction which turns a class of
                 objects into a generic object. It is suggested that all
                 objects (individual, aggregate, generic) should be
                 given uniform treatment in models of the real world. A
                 new data type, called generic, is developed as a
                 primitive for defining such models. Models defined with
                 this primitive are structured as a set of aggregation
                 hierarchies intersecting with a set of generalization
                 hierarchies. Abstract objects occur at the points of
                 intersection. This high level structure provides a
                 discipline for the organization of relational
                 databases. In particular this discipline allows: (i) an
                 important class of views to be integrated and
                 maintained; (ii) stability of data and programs under
                 certain evolutionary changes; (iii) easier
                 understanding of complex models and more natural {\em
                 query formulation;\/} (iv) {\em a more systematic
                 approach to database design;\/} (v) {\em more
                 optimization\/} to be performed at lower implementation
                 levels. The generic type is formalized by a set of
                 invariant properties. These properties should be
                 satisfied by all relations in a database if
                 abstractions are to be preserved. A triggering
                 mechanism for automatically maintaining these
                 invariants during update operations is proposed. A
                 simple mapping of aggregation/generalization
                 hierarchies onto owner-coupled set structures is
                 given.",
  acknowledgement = ack-nhfb,
  keywords =     "aggregation; data abstraction; data model; data type;
                 database design; dblit data abstraction;
                 generalization; integrity constraints; knowledge
                 representation; relational database",
  subject =      "Information Systems --- Database Management ---
                 Logical Design (H.2.1): {\bf Data models}; Information
                 Systems --- Database Management --- Systems (H.2.4):
                 {\bf Relational databases}; Software --- Software
                 Engineering --- Software Architectures (D.2.11): {\bf
                 Data abstraction}",
}

@Article{Shu:1977:EDE,
  author =       "N. C. Shu and B. C. Housel and R. W. Taylor and S. P.
                 Ghosh and V. Y. Lum",
  title =        "{EXPRESS}: a data {EXtraction, Processing, and
                 Restructuring System}",
  journal =      j-TODS,
  volume =       "2",
  number =       "2",
  pages =        "134--174",
  month =        jun,
  year =         "1977",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Database/Graefe.bib; http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1977-2-2/p134-shu/p134-shu.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1977-2-2/p134-shu/",
  abstract =     "EXPRESS is an experimental prototype data translation
                 system which can access a wide variety of data and
                 restructure it for new uses. The system is driven by
                 two very high level nonprocedural languages: DEFINE for
                 data description and CONVERT for data restructuring.
                 Program generation and cooperating process techniques
                 are used to achieve efficient operation.\par

                 This paper describes the design and implementation of
                 EXPRESS. DEFINE and CONVERT are summarized and the
                 implementation architecture presented.\par

                 The DEFINE description is compiled into a customized
                 PL/1 program for accessing source data. The
                 restructuring specified in CONVERT is compiled into a
                 set of customized PL/1 procedures to derive multiple
                 target files from multiple input files. Job steps and
                 job control statements are generated automatically.
                 During execution, the generated procedures run under
                 control of a process supervisor, which coordinates
                 buffer management and handles file allocation,
                 deallocation, and all input/output requests.\par

                 The architecture of EXPRESS allows efficiency in
                 execution by avoiding unnecessary secondary storage
                 references while at the same time allowing the
                 individual procedures to be independent of each other.
                 Its modular structure permits the system to be extended
                 or transferred to another environment easily.",
  acknowledgement = ack-nhfb,
  keywords =     "data conversion; data description languages; data
                 manipulation languages; data restructuring; data
                 translation; file conversion; program generation; very
                 high level languages",
  subject =      "Information Systems --- Database Management ---
                 Heterogeneous Databases (H.2.5): {\bf Data
                 translation**}; Information Systems --- Information
                 Storage and Retrieval --- Information Storage (H.3.2):
                 {\bf File organization}; Information Systems ---
                 Database Management --- Languages (H.2.3)",
}

@Article{Ozkarahan:1977:PER,
  author =       "E. A. Ozkarahan and S. A. Schuster and K. C. Sevcik",
  title =        "Performance Evaluation of a Relational Associative
                 Processor",
  journal =      j-TODS,
  volume =       "2",
  number =       "2",
  pages =        "175--195",
  month =        jun,
  year =         "1977",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Database/Graefe.bib; http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1977-2-2/p175-ozkarahan/p175-ozkarahan.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1977-2-2/p175-ozkarahan/",
  abstract =     "An associative processor called RAP has been designed
                 to provide hardware support for the use and
                 manipulation of databases. RAP is particularly suited
                 for supporting relational databases. In this paper, the
                 relational operations provided by the RAP hardware are
                 described, and a representative approach to providing
                 the same relational operations with conventional
                 software and hardware is devised. Analytic models are
                 constructed for RAP and the conventional system. The
                 execution times of several of the operations are shown
                 to be vastly improved with RAP for large relations.",
  acknowledgement = ack-nhfb,
  keywords =     "associative processors; database machines; performance
                 evaluation; RAP hardware support database machine TODS;
                 relational databases",
  subject =      "Information Systems --- Database Management ---
                 Systems (H.2.4): {\bf Relational databases}; Hardware
                 --- Control Structures and Microprogramming --- Control
                 Structure Performance Analysis and Design Aids
                 (B.1.2)",
}

@Article{Brice:1977:EPD,
  author =       "Richard S. Brice and Stephen W. Sherman",
  title =        "An Extension on the Performance of a Database Manager
                 in a Virtual Memory System Using Partially Locked
                 Virtual Buffers",
  journal =      j-TODS,
  volume =       "2",
  number =       "2",
  pages =        "196--207",
  month =        jun,
  year =         "1977",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Database/Graefe.bib; http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1977-2-2/p196-brice/p196-brice.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1977-2-2/p196-brice/",
  abstract =     "Buffer pools are created and managed in database
                 systems in order to reduce the total number of accesses
                 to the I/O devices. In systems using virtual memory,
                 any reduction in I/O accesses may be accompanied by an
                 increase in paging. The effects of these factors on
                 system performance are quantified, where system
                 performance is a function of page faults and database
                 accesses to the I/O devices. A previous study of this
                 phenomenon is extended through the analysis of
                 empirical data gathered in a multifactor experiment. In
                 this study memory is partitioned between the program
                 and the buffer so that the impact of the controlled
                 factors can be more effectively evaluated. It is
                 possible to improve system performance through the use
                 of different paging algorithms in the program partition
                 and the buffer partition. Also, the effects on system
                 performance as the virtual buffer size is increased
                 beyond the real memory allocated to the buffer
                 partition are investigated.",
  acknowledgement = ack-nhfb,
  keywords =     "buffer manager; database management; double paging;
                 locked buffer; page faults; page replacement algorithm;
                 performance; pinning fixing TODS; virtual buffer;
                 virtual memory",
  subject =      "Hardware --- Control Structures and Microprogramming
                 --- Control Structure Performance Analysis and Design
                 Aids (B.1.2); Information Systems --- Database
                 Management --- Systems (H.2.4): {\bf Database
                 Manager}",
}

@Article{Lohman:1977:OPB,
  author =       "Guy M. Lohman and John A. Muckstadt",
  title =        "Optimal Policy for Batch Operations: Backup,
                 Checkpointing, Reorganization, and Updating",
  journal =      j-TODS,
  volume =       "2",
  number =       "3",
  pages =        "209--222",
  month =        sep,
  year =         "1977",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Compendex database; Database/Graefe.bib;
                 Database/Wiederhold.bib; http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1977-2-3/p209-lohman/p209-lohman.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1977-2-3/p209-lohman/",
  abstract =     "Many database maintenance operations are performed
                 periodically in batches, even in realtime systems. The
                 purpose of this paper is to present a general model for
                 determining the optimal frequency of these batch
                 operations. Specifically, optimal backup,
                 checkpointing, batch updating, and reorganization
                 policies are derived. The approach used exploits
                 inventory parallels by seeking the optimal number of
                 items--rather than a time interval--to trigger a batch.
                 The Renewal Reward Theorem is used to find the average
                 long run costs for backup, recovery, and item storage,
                 per unit time, which is then minimized to find the
                 optimal backup policy. This approach permits far less
                 restrictive assumptions about the update arrival
                 process than did previous models, as well as inclusion
                 of storage costs for the updates. The optimal
                 checkpointing, batch updating, and reorganization
                 policies are shown to be special cases of this optimal
                 backup policy. The derivation of previous results as
                 special cases of this model, and an example,
                 demonstrate the generality of the methodology
                 developed.",
  acknowledgement = ack-nhfb,
  classification = "723",
  keywords =     "backup frequency; batch operations; batch update;
                 checkpoint interval; data base systems; database
                 maintenance; file reorganization; inventory theory;
                 real-time systems; renewal theory",
  subject =      "Information Systems --- Database Management ---
                 General (H.2.0)",
}

@Article{Wong:1977:IHT,
  author =       "Kai C. Wong and Murray Edelberg",
  title =        "Interval Hierarchies and Their Application to
                 Predicate Files",
  journal =      j-TODS,
  volume =       "2",
  number =       "3",
  pages =        "223--232",
  month =        sep,
  year =         "1977",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Compendex database; Database/Graefe.bib;
                 http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1977-2-3/p223-wong/p223-wong.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1977-2-3/p223-wong/",
  abstract =     "Predicates are used extensively in modern database
                 systems for purposes ranging from user specification of
                 associative accesses to data, to user-invisible system
                 control functions such as concurrency control and data
                 distribution. Collections of predicates, or predicate
                 files, must be maintained and accessed efficiently. A
                 dynamic index is described, called an interval
                 hierarchy, which supports several important retrieval
                 operations on files of simple conjunctive predicates.
                 Search and maintenance algorithms for interval
                 hierarchies are given. For a file of n predicates,
                 typical of the kind expected in practice, these
                 algorithms require time equal to $O(\log n)$.",
  acknowledgement = ack-nhfb,
  classification = "723",
  keywords =     "concurrency control; data base systems; database
                 system; distributed data; index; interval; predicate
                 file",
  subject =      "Software --- Operating Systems --- Storage Management
                 (D.4.2): {\bf Storage hierarchies}; Information Systems
                 --- Information Storage and Retrieval --- Information
                 Storage (H.3.2): {\bf File organization}; Information
                 Systems --- Database Management --- Systems (H.2.4):
                 {\bf Distributed databases}; Information Systems ---
                 Database Management --- Systems (H.2.4): {\bf
                 Concurrency}; Information Systems --- Information
                 Storage and Retrieval --- Content Analysis and Indexing
                 (H.3.1): {\bf Indexing methods}",
}

@Article{Ries:1977:ELG,
  author =       "Daniel R. Ries and Michael Stonebraker",
  title =        "Effects of Locking Granularity in a Database
                 Management System",
  journal =      j-TODS,
  volume =       "2",
  number =       "3",
  pages =        "233--246",
  month =        sep,
  year =         "1977",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Compendex database; Database/Graefe.bib;
                 Database/Wiederhold.bib; http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1977-2-3/p233-ries/p233-ries.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1977-2-3/p233-ries/",
  abstract =     "Many database systems guarantee some form of integrity
                 control upon multiple concurrent updates by some form
                 of locking. Some ``granule'' of the database is chosen
                 as the unit which is individually locked, and a lock
                 management algorithm is used to ensure integrity. Using
                 a simulation model, this paper explores the desired
                 size of a granule. Under a wide variety of seemingly
                 realistic conditions, surprisingly coarse granularity
                 is called for. The paper concludes with some
                 implications of these results concerning the viability
                 of so-called predicate locking.",
  acknowledgement = ack-nhfb,
  classification = "723",
  keywords =     "concurrency; consistency; data base systems; database
                 management; locking granularity; multiple updates;
                 predicate locks",
  subject =      "Information Systems --- Database Management ---
                 Systems (H.2.4); Information Systems --- Database
                 Management (H.2); Information Systems --- Database
                 Management --- Systems (H.2.4): {\bf Concurrency}",
}

@Article{Schmidt:1977:SHL,
  author =       "Joachim W. Schmidt",
  title =        "Some High Level Language Constructs for Data of Type
                 Relation",
  journal =      j-TODS,
  volume =       "2",
  number =       "3",
  pages =        "247--261",
  month =        sep,
  year =         "1977",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Compendex database; Database/Graefe.bib;
                 Database/Wiederhold.bib; http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1977-2-3/p247-schmidt/p247-schmidt.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1977-2-3/p247-schmidt/",
  abstract =     "For the extension of high level languages by data
                 types of mode relation, three language constructs are
                 proposed and discussed: a repetition statement
                 controlled by relations, predicates as a generalization
                 of Boolean expressions, and a constructor for relations
                 using predicates. The language constructs are developed
                 step by step starting with a set of elementary
                 operations on relations. They are designed to fit into
                 PASCAL without introducing too many additional
                 concepts.",
  acknowledgement = ack-nhfb,
  annote =       "PASCAL/R",
  classification = "723",
  keywords =     "computer programming languages; data type; database;
                 high level language; language extension; nonprocedural
                 language; relational calculus; relational model",
  subject =      "Information Systems --- Database Management ---
                 Systems (H.2.4): {\bf Relational databases};
                 Information Systems --- Database Management ---
                 Languages (H.2.3)",
}

@Article{Fagin:1977:MVD,
  author =       "Ronald Fagin",
  title =        "Multi-Valued Dependencies and a New Normal Form for
                 Relational Databases",
  journal =      j-TODS,
  volume =       "2",
  number =       "3",
  pages =        "262--278",
  month =        sep,
  year =         "1977",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Compendex database; Database/Graefe.bib;
                 Database/Wiederhold.bib; Distributed/gesturing.bib;
                 http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1977-2-3/p262-fagin/p262-fagin.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1977-2-3/p262-fagin/",
  abstract =     "A new type of dependency, which includes the
                 well-known functional dependencies as a special case,
                 is defined for relational databases. By using this
                 concept, a new (``fourth'') normal form for relation
                 schemata is defined. This fourth normal form is
                 strictly stronger than Codd's ``improved third normal
                 form'' (or ``Boyce-Codd normal form''). It is shown
                 that every relation schema can be decomposed into a
                 family of relation schemata in fourth normal form
                 without loss of information (that is, the original
                 relation can be obtained from the new relations by
                 taking joins).",
  acknowledgement = ack-nhfb,
  annote =       "Multivalued dependency is defined for relational
                 databases, a new (``fourth'') normal form is strictly
                 stronger than Codd's.",
  classification = "723",
  keywords =     "3NF; 4NF; Boyce-Codd normal form; data base systems;
                 database design; decomposition; fourth normal form;
                 functional dependency; multivalued dependency;
                 normalization; relational database; third normal form",
  subject =      "Information Systems --- Database Management ---
                 Systems (H.2.4): {\bf Relational databases};
                 Information Systems --- Database Management --- Logical
                 Design (H.2.1): {\bf Normal forms}",
}

@Article{March:1977:DER,
  author =       "Salvatore T. March and Dennis G. Severance",
  title =        "The Determination of Efficient Record Segmentations
                 and Blocking Factors for Shared Data Files",
  journal =      j-TODS,
  volume =       "2",
  number =       "3",
  pages =        "279--296",
  month =        sep,
  year =         "1977",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Compendex database; Database/Graefe.bib;
                 Database/Wiederhold.bib; http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1977-2-3/p279-march/p279-march.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1977-2-3/p279-march/",
  abstract =     "It is generally believed that 80 percent of all
                 retrieval from a commercial database is directed at
                 only 20 percent of the stored data items. By
                 partitioning data items into primary and secondary
                 record segments, storing them in physically separate
                 files, and judiciously allocating available buffer
                 space to the two files, it is possible to significantly
                 reduce the average cost of information retrieval from a
                 shared database. An analytic model, based upon
                 knowledge of data item lengths, data access costs, and
                 user retrieval patterns, is developed to assist an
                 analyst with this assignment problem. A computationally
                 tractable design algorithm is presented and results of
                 its application are described.",
  acknowledgement = ack-nhfb,
  classification = "723; 901",
  keywords =     "bicriterion mathematical programs; branch and bound;
                 buffer allocation; data base systems; data management;
                 information science --- information retrieval; network
                 flows; record design; record segmentation",
  subject =      "Information Systems --- Database Management (H.2);
                 Information Systems --- Information Storage and
                 Retrieval --- Information Storage (H.3.2): {\bf File
                 organization}",
}

@Article{Ozkarahan:1977:AAF,
  author =       "E. A. Ozkarahan and K. C. Sevcik",
  title =        "Analysis of Architectural Features for Enhancing the
                 Performance of a Database Machine",
  journal =      j-TODS,
  volume =       "2",
  number =       "4",
  pages =        "297--316",
  month =        dec,
  year =         "1977",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Compendex database; Database/Graefe.bib;
                 Database/Wiederhold.bib; http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1977-2-4/p297-ozkarahan/p297-ozkarahan.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1977-2-4/p297-ozkarahan/",
  abstract =     "RAP (Relational Associative Processor) is a
                 ``back-end'' database processor that is intended to
                 take over much of the effort of database management in
                 a computer system. In order to enhance RAP's
                 performance its design includes mechanisms for
                 permitting features analogous to multiprogramming and
                 virtual memory as in general purpose computer systems.
                 It is the purpose of this paper to present the detailed
                 design of these mechanisms, along with some analysis
                 that supports their value. Specifically, (1) the
                 response time provided by RAP under several scheduling
                 disciplines involving priority by class is analyzed,
                 (2) the cost effectiveness of the additional hardware
                 in RAP necessary to support multiprogramming is
                 assessed, and (3) a detailed design of the RAP virtual
                 memory system and its monitor is presented.",
  acknowledgement = ack-nhfb,
  annote =       "RAP (Relational Associative Processor) is a ``back-end
                 database processor''; its design includes mechanisms
                 for multiprogramming and virtual memory.",
  classification = "722; 723",
  keywords =     "associative processors; computer architecture;
                 computer architecture, hardware support TODS; data base
                 systems; database machines; database management",
  subject =      "Information Systems --- Database Management (H.2)",
}

@Article{Rissanen:1977:ICR,
  author =       "Jorma Rissanen",
  title =        "Independent Components of Relations",
  journal =      j-TODS,
  volume =       "2",
  number =       "4",
  pages =        "317--325",
  month =        dec,
  year =         "1977",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Compendex database; Database/Graefe.bib;
                 Database/Wiederhold.bib; http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1977-2-4/p317-rissanen/p317-rissanen.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1977-2-4/p317-rissanen/",
  abstract =     "In a multiattribute relation or, equivalently, a
                 multicolumn table a certain collection of the
                 projections can be shown to be independent in much the
                 same way as the factors in a Cartesian product or
                 orthogonal components of a vector. A precise notion of
                 independence for relations is defined and studied. The
                 main result states that the operator which reconstructs
                 the original relation from its independent components
                 is the natural join, and that independent components
                 split the full family of functional dependencies into
                 corresponding component families. These give an
                 easy-to-check criterion for independence.",
  acknowledgement = ack-nhfb,
  annote =       "In a multi-attribute relation a certain collection of
                 projections can be shown to be independent. The
                 operator which reconstructs the original relation is
                 the natural join. Independent components split the full
                 family of functional dependencies into corresponding
                 component families.",
  classification = "723",
  keywords =     "data base systems; database; functional dependencies;
                 relations",
  subject =      "Information Systems --- Database Management ---
                 Systems (H.2.4): {\bf Relational databases}",
}

@Article{Bonczek:1977:TGB,
  author =       "Robert H. Bonczek and James I. Cash and Andrew B.
                 Whinston",
  title =        "A Transformational Grammar-Based Query Processor for
                 Access Control in a Planning System",
  journal =      j-TODS,
  volume =       "2",
  number =       "4",
  pages =        "326--338",
  month =        dec,
  year =         "1977",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Compendex database; Database/Graefe.bib;
                 http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1977-2-4/p326-bonczek/p326-bonczek.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1977-2-4/p326-bonczek/",
  abstract =     "Providing computer facilities and data availability to
                 larger numbers of users generates increased system
                 vulnerability which is partially offset by software
                 security systems. Much too often these systems are
                 presented as ad hoc additions to the basic data
                 management system. One very important constituent of
                 software security systems is the access control
                 mechanism which may be the last resource available to
                 prohibit unauthorized data retrieval. This paper
                 presents a specification for an access control
                 mechanism. The mechanism is specified in a context for
                 use with the GPLAN decision support system by a
                 theoretical description consistent with the formal
                 definition of GPLAN's query language. Incorporation of
                 the mechanism into the language guarantees it will not
                 be an ad hoc addition. Furthermore, it provides a
                 facile introduction of data security dictates into the
                 language processor.",
  acknowledgement = ack-nhfb,
  classification = "723",
  keywords =     "access control; data processing; data security;
                 database; decision support system; planning system",
  subject =      "Information Systems --- Database Management ---
                 Systems (H.2.4): {\bf Query processing}; Information
                 Systems --- Database Management --- General (H.2.0):
                 {\bf Security, integrity, and protection**};
                 Information Systems --- Database Management ---
                 Physical Design (H.2.2): {\bf Access methods}",
}

@Article{Lang:1977:DBP,
  author =       "Tom{\'a}s Lang and Christopher Wood and Eduardo B.
                 Fern{\'a}ndez",
  title =        "Database Buffer Paging in Virtual Storage Systems",
  journal =      j-TODS,
  volume =       "2",
  number =       "4",
  pages =        "339--351",
  month =        dec,
  year =         "1977",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Compendex database; Database/Graefe.bib;
                 Database/Wiederhold.bib; http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1977-2-4/p339-lang/p339-lang.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1977-2-4/p339-lang/",
  abstract =     "Three models, corresponding to different sets of
                 assumptions, are analyzed to study the behavior of a
                 database buffer in a paging environment. The models
                 correspond to practical situations and vary in their
                 search strategies and replacement algorithms. The
                 variation of I/O cost with respect to buffer size is
                 determined for the three models. The analysis is valid
                 for arbitrary database and buffer sizes, and the I/O
                 cost is obtained in terms of the miss ratio, the buffer
                 size, the number of main memory pages available for the
                 buffer, and the relative buffer and database access
                 costs.",
  acknowledgement = ack-nhfb,
  annote =       "The variation of I/O cost with respect to buffer size
                 is determined for three models: the IMS/360 database
                 buffer, with LRU memory replacement, and a prefix table
                 in main memory indicating which database pages are in
                 the VSAM buffer.",
  classification = "723",
  keywords =     "buffer management; computer systems performance; data
                 base systems; database performance; page replacement
                 algorithm; virtual memory",
  subject =      "Information Systems --- Database Management ---
                 General (H.2.0); Information Systems --- Database
                 Management --- Systems (H.2.4)",
}

@Article{Thomas:1977:VAP,
  author =       "D. A. Thomas and B. Pagurek and R. J. Buhr",
  title =        "Validation Algorithms for Pointer Values in {DBTG}
                 Databases",
  journal =      j-TODS,
  volume =       "2",
  number =       "4",
  pages =        "352--369",
  month =        dec,
  year =         "1977",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Compendex database; Database/Graefe.bib;
                 Database/Wiederhold.bib; http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1977-2-4/p352-thomas/p352-thomas.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1977-2-4/p352-thomas/",
  abstract =     "This paper develops algorithms for verifying pointer
                 values in DBTG (Data Base Task Group) type databases.
                 To validate pointer implemented access paths and set
                 structures, two algorithms are developed. The first
                 procedure exploits the ``typed pointer'' concept
                 employed in modern programming languages to diagnose
                 abnormalities in directories and set instances. The
                 second algorithm completes pointer validation by
                 examining set instances to ensure that each DBTG set
                 has a unique owner. Sequential processing is used by
                 both algorithms, allowing a straightforward
                 implementation which is efficient in both time and
                 space. As presented, the algorithms are independent of
                 implementation schema and physical structure.",
  acknowledgement = ack-nhfb,
  annote =       "Type Checking algorithm detects and locates errors in
                 the pointers which are used to represent chained and
                 pointer array implemented sets. In addition to invalid
                 set pointers, the algorithm has been extended to check
                 index sequential and inverted access directories
                 provided by EDMS.",
  classification = "723",
  keywords =     "data base systems; database integrity; database
                 utilities; type checking; validation",
  subject =      "Information Systems --- Database Management (H.2);
                 Information Systems --- Database Management --- General
                 (H.2.0): {\bf Security, integrity, and protection**}",
}

@Article{Claybrook:1977:FDM,
  author =       "Billy G. Claybrook",
  title =        "A Facility for Defining and Manipulating Generalized
                 Data Structures",
  journal =      j-TODS,
  volume =       "2",
  number =       "4",
  pages =        "370--406",
  month =        dec,
  year =         "1977",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Compendex database; Database/Graefe.bib;
                 http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1977-2-4/p370-claybrook/p370-claybrook.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1977-2-4/p370-claybrook/",
  abstract =     "A data structure definition facility (DSDF) is
                 described that provides definitions for several
                 primitive data types, homogeneous and heterogeneous
                 arrays, cells, stacks, queues, trees, and general
                 lists. Each nonprimitive data structure consists of two
                 separate entities--a head and a body. The head contains
                 the entry point(s) to the body of the structure; by
                 treating the head like a cell, the DSDF operations are
                 capable of creating and manipulating very general data
                 structures. A template structure is described that
                 permits data structures to share templates.\par

                 The primary objectives of the DSDF are: (1) to develop
                 a definition facility that permits the programmer to
                 explicitly define and manipulate generalized data
                 structures in a consistent manner, (2) to detect
                 mistakes and prevent the programmer from creating
                 (either inadvertently or intentionally) undesirable (or
                 illegal) data structures, (3) to provide a syntactic
                 construction mechanism that separates the
                 implementation of a data structure from its use in the
                 program in which it is defined, and (4) to facilitate
                 the development of reliable software.",
  acknowledgement = ack-nhfb,
  classification = "723",
  keywords =     "data definition languages; data processing; data
                 structure definition facility; data structures;
                 database management",
  subject =      "Information Systems --- Database Management (H.2);
                 Information Systems --- Database Management ---
                 Languages (H.2.3)",
}

@Article{Minker:1978:SSS,
  author =       "Jack Minker",
  title =        "Search Strategy and Selection Function for an
                 Inferential Relational System",
  journal =      j-TODS,
  volume =       "3",
  number =       "1",
  pages =        "1--31",
  month =        mar,
  year =         "1978",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Compendex database; Database/Graefe.bib;
                 Database/Wiederhold.bib; http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1978-3-1/p1-minker/p1-minker.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1978-3-1/p1-minker/",
  abstract =     "An inferential relational system is one in which data
                 in the system consists of both explicit facts and
                 general axioms (or ``views''). The general axioms are
                 used together with the explicit facts to derive the
                 facts that are implicit (virtual relations) within the
                 system. A top-down algorithm, as used in artificial
                 intelligence work, is described to develop inferences
                 within the system. The top-down approach starts with
                 the query, a conjunction of relations, to be answered.
                 Either a relational fact solves a given relation in a
                 conjunct, or the relation is replaced by a conjunct of
                 relations which must be solved to solve the given
                 relation. The approach requires that one and only one
                 relation in a conjunction be replaced (or expanded) by
                 the given facts and general axioms. The decision to
                 expand only a single relation is termed a selection
                 function. It is shown for relational systems that such
                 a restriction still guarantees that a solution to the
                 problem will be found if one exists.\par

                 The algorithm provides for heuristic direction in the
                 search process. Experimental results are presented
                 which illustrate the techniques. A bookkeeping
                 mechanism is described which permits one to know when
                 subproblems are solved. It further facilitates the
                 outputting of reasons for the deductively found answer
                 in a coherent fashion.",
  acknowledgement = ack-nhfb,
  annote =       "Data in the system consists of both explicit facts and
                 general axioms. The top-down approach starts with the
                 query, a conjunction of relations, to be answered.
                 Either a relational fact solves a given relation in a
                 conjunct, or the relation is replaced by a conjunct of
                 relations which must be solved to solve the given
                 relation. Experimental results are presented which
                 illustrate the techniques.",
  classification = "723",
  keywords =     "answer and reason extraction; data base systems;
                 heuristics; inference mechanism; logic; predicate
                 calculus; relational databases; search strategy;
                 selection function; top-down search; virtual
                 relations",
  subject =      "Information Systems --- Database Management ---
                 Systems (H.2.4): {\bf Relational databases};
                 Information Systems --- Information Storage and
                 Retrieval --- Information Search and Retrieval (H.3.3):
                 {\bf Search process}",
}

@Article{Tuel:1978:ORP,
  author =       "William G. {Tuel, Jr.}",
  title =        "Optimum Reorganization Points for Linearly Growing
                 Files",
  journal =      j-TODS,
  volume =       "3",
  number =       "1",
  pages =        "32--40",
  month =        mar,
  year =         "1978",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Compendex database; Database/Graefe.bib;
                 Database/Wiederhold.bib; http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1978-3-1/p32-tuel/p32-tuel.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1978-3-1/p32-tuel/",
  abstract =     "The problem of finding optimal reorganization
                 intervals for linearly growing files is solved. An
                 approximate reorganization policy, independent of file
                 lifetime, is obtained. Both the optimum and approximate
                 policies are compared to previously published results
                 using a numerical example.",
  acknowledgement = ack-nhfb,
  annote =       "The problem of finding optimal reorganization
                 intervals for linearly growing files is solved.",
  classification = "723",
  keywords =     "data processing --- file organization; database; file
                 organization; optimization; physical database design
                 TODS, data base systems; reorganization",
  subject =      "Information Systems --- Information Storage and
                 Retrieval --- Information Storage (H.3.2): {\bf File
                 organization}",
}

@Article{Yu:1978:END,
  author =       "C. T. Yu and W. S. Luk and M. K. Siu",
  title =        "On the Estimation of the Number of Desired Records
                 with Respect to a Given Query",
  journal =      j-TODS,
  volume =       "3",
  number =       "1",
  pages =        "41--56",
  month =        mar,
  year =         "1978",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Compendex database; Database/Graefe.bib;
                 Database/Wiederhold.bib; http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1978-3-1/p41-yu/p41-yu.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1978-3-1/p41-yu/",
  abstract =     "The importance of the estimation of the number of
                 desired records for a given query is outlined. Two
                 algorithms for the estimation in the ``closest
                 neighbors problem'' are presented. The numbers of
                 operations of the algorithms are $O(m\ell^2)$ and
                 $O(m\ell)$, where $m$ is the number of clusters and
                 $\ell$ is the ``length'' of the query.",
  acknowledgement = ack-nhfb,
  annote =       "Two Algorithms for the estimation in the `closest
                 neighbors problem'",
  classification = "901",
  keywords =     "closest neighbors; database; estimate; information
                 science, CTYu selectivity TODS; query",
  subject =      "Information Systems --- Database Management ---
                 Systems (H.2.4): {\bf Query processing}",
}

@Article{Su:1978:CCS,
  author =       "Stanley Y. W. Su and Ahmed Emam",
  title =        "{CASDAL}: {{\em CAS\/}SM}'s {{\em DA\/}}ta {{\em
                 L\/}}anguage",
  journal =      j-TODS,
  volume =       "3",
  number =       "1",
  pages =        "57--91",
  month =        mar,
  year =         "1978",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Compendex database; Database/Graefe.bib;
                 Database/Wiederhold.bib; http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/citations/journals/tods/1978-3-1/p57-su/",
  abstract =     "CASDAL is a high level data language designed and
                 implemented for the database machine CASSM. The
                 language is used for the manipulation and maintenance
                 of a database using an unnormalized (hierarchically
                 structured) relational data model. It also has
                 facilities to define, modify, and maintain the data
                 model definition. The uniqueness of CASDAL lies in its
                 power to specify complex operations in terms of several
                 new language constructs and its concepts of tagging or
                 marking tuples and of matching values when walking from
                 relation to relation. The language is a result of a
                 top-down design and development effort for a database
                 machine in which high level language constructs are
                 directly supported by the hardware. This paper (1)
                 gives justifications for the use of an unnormalized
                 relational model on which the language is based, (2)
                 presents the CASDAL language constructs with examples,
                 and (3) describes CASSM's architecture and hardware
                 primitives which match closely with the high level
                 language constructs and facilitate the translation
                 process. This paper also attempts to show how the
                 efficiency of the language and the translation task can
                 be achieved and simplified in a system in which the
                 language is the result of a top-down system design and
                 development.",
  acknowledgement = ack-nhfb,
  annote =       "CASDAL is a high level data language for the database
                 machine CASSM. It uses an unnormalized (hierarchically
                 structured) relational data model. This paper (1)
                 justifies the use of this model (2) presents the Casdal
                 language constructs with examples, and (3) describes
                 CASSM's architecture.",
  classification = "723",
  keywords =     "associative memory; computer programming languages;
                 data language; database; nonprocedural language; query
                 language; relational model; SYWSu hardware support
                 database machine TODS, data base systems",
  subject =      "Information Systems --- Database Management ---
                 Languages (H.2.3); Information Systems --- Database
                 Management --- Languages (H.2.3): {\bf Query
                 languages}; Information Systems --- Database Management
                 --- Systems (H.2.4): {\bf Relational databases}",
}

@Article{Chin:1978:SSD,
  author =       "Francis Y. Chin",
  title =        "Security in Statistical Databases for Queries with
                 Small Counts",
  journal =      j-TODS,
  volume =       "3",
  number =       "1",
  pages =        "92--104",
  month =        mar,
  year =         "1978",
  CODEN =        "ATDSD3",
  ISSN =         "0362-5915",
  bibdate =      "Sat Apr 14 10:34:48 MDT 2001",
  bibsource =    "Compendex database; Database/Graefe.bib;
                 Database/Wiederhold.bib; http://www.acm.org/pubs/toc/",
  URL =          "http://www.acm.org/pubs/articles/journals/tods/1978-3-1/p92-chin/p92-chin.pdf;
                 http://www.acm.org/pubs/citations/journals/tods/1978-3-1/p92-chin/",
  abstract =     "The security problem of statistical databases
                 containing anonymous but individual records which may
                 be evaluated by queries about sums and averages is
                 considered. A model, more realistic than the previous
                 ones, is proposed, in which nonexisting records for
                 some keys can be allowed. Under the assumption that the
                 system protects the individual's information by the
                 well-known technique which avoids publishing summaries
                 with small counts, several properties about the system
                 and a necessary and sufficient condition for
                 compromising the database have been derived. The
                 minimum number of queries needed to compromise the
                 database is also discussed.",
  acknowledgement = ack-nhfb,
  annote =       "Under the assumption that the system protects the
                 individual's information by the technique which avoids
                 publishing summaries with small counts, properties
                 about the system and a necessary and sufficient
                 condition for compromising the database have been
                 derived.",
  classification = "723",
  keywords =     "compromisability; data base systems; data processing
                 --- security of data; data security; protection;
                 statistical databases",
  subject =      "Information Systems --- Database Management ---
                 Database Applications (H.2.8): {\bf Statistical
                 databases}; Information Systems --- Database Management
                 --- Systems (H.2.4): {\bf Query processing};
                 Information Systems --- Database Management ---
                 Database Administration (H.2.7): {\bf Security,
                 integrity, and protection}",
}
